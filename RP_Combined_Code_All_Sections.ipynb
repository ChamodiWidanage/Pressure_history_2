{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Importing libraries and files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "498e1bf0a688a38b"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_20808/758049542.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodels\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mload_model\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mpickle\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpyplot\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mplt\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mio\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhttps\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m//\u001B[0m\u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mio\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m \"\"\"\n\u001B[1;32m---> 20\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mkeras\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdistribute\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     21\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mkeras\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mmodels\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minput_layer\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mInput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\distribute\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 18\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mkeras\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdistribute\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0msidecar_evaluator\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\distribute\\sidecar_evaluator.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[1;34m\"\"\"Python module for evaluation loop.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 17\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcompat\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mv2\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mtf\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     18\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;31m# isort: off\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "ann_arr = load_model('RP_Arrival_time/Arrival_ANN4')\n",
    "ann_end = load_model('RP_End_time/End_ANN1')\n",
    "ann_chg = load_model('RP_Change_time/Change_ANN2')\n",
    "ann_trg = load_model('RP_Trough_time/Trough_ANN2')\n",
    "\n",
    "#Section1\n",
    "ann1 = load_model('RP_Section_1_new3/Section1_new3_ANN_2')\n",
    "sc1 = pickle.load(open('RP_Section_1_new3/Section1_new3_scaler_ANN_2.pkl', 'rb'))\n",
    "#Section2\n",
    "ann2 = load_model('RP_Section_2_new2/Section2_new_ANN_2')\n",
    "sc2 = pickle.load(open('RP_Section_2_new/Section2_new_scaler_ANN_2.pkl', 'rb'))\n",
    "#Section3\n",
    "ann3 = load_model('RP_Section_3_new/Section3_new_ANN_2')\n",
    "sc3 = pickle.load(open('RP_Section_3_new/Section3_new_scaler_ANN_2.pkl', 'rb'))\n",
    "#Section4\n",
    "ann4 = load_model('RP_Section_4_new/Section4_new_ANN_7')\n",
    "sc4 = pickle.load(open('RP_Section_4_new/Section4_new_scaler_ANN_7.pkl', 'rb'))\n",
    "#Section5\n",
    "ann5 = load_model('RP_Section_5_new/Section5_new_ANN_4')\n",
    "sc5 = pickle.load(open('RP_Section_5_new/Section5_new_scaler_ANN_4.pkl', 'rb'))\n",
    "#Section6\n",
    "ann6 = load_model('RP_Section_6_new/Section6_new_ANN_2')\n",
    "sc6 = pickle.load(open('RP_Section_6_new/Section6_new_scaler_ANN_2.pkl', 'rb'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-26T00:12:20.295481300Z",
     "start_time": "2024-08-26T00:12:18.896911400Z"
    }
   },
   "id": "3122126ae010792f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Input details"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f559d0fa330bc79"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Mass = 17.3\n",
    "Standoff_distance = 4.5\n",
    "Angle = 5\n",
    "termination_time = 40\n",
    "interval = 0.1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "799ba05d5cac9d28"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pressure Profile"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4682781d5ba61ce7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#values1 = []\n",
    "#values2 = []\n",
    "#values3 = []\n",
    "arr = np.array ([[Mass,Standoff_distance, Angle]])\n",
    "\n",
    "arrival_time = ann_arr.predict(arr)\n",
    "end_time = ann_end.predict(arr)\n",
    "change_time = ann_chg.predict(arr)\n",
    "trough_time = ann_trg.predict(arr)\n",
    "\n",
    "print (\"arrival\", arrival_time, \"end\", end_time, \"change\", change_time, \"trough\", trough_time)\n",
    "\n",
    "def stable_state(start, end, interval):    \n",
    "    t = np.arange(start, end + interval, interval)\n",
    "    t = t.reshape(len(t),1)\n",
    "\n",
    "    p = np.repeat(0, len(t)).reshape(len(t),1)\n",
    "    \n",
    "    values = np.concatenate ((t, p), axis = 1)\n",
    "    return values\n",
    "\n",
    "def p_fluctuation(standoff_distance, d_set):\n",
    "    if 1.5 <= standoff_distance <= 3:\n",
    "        d_set[:,0:3] = sc1.transform(d_set[:,0:3])\n",
    "        p_f = ann1.predict(d_set)\n",
    "        \n",
    "    elif 3 < standoff_distance <= 7:\n",
    "        d_set[:,0:3] = sc2.transform(d_set[:,0:3])\n",
    "        p_f = ann2.predict(d_set)\n",
    "        \n",
    "    elif 7 < standoff_distance <= 13:\n",
    "        d_set[:,0:3] = sc3.transform(d_set[:,0:3])\n",
    "        p_f = ann3.predict(d_set)\n",
    "        \n",
    "    elif 13 < standoff_distance <= 19:\n",
    "        d_set[:,0:3] = sc4.transform(d_set[:,0:3])\n",
    "        p_f = ann4.predict(d_set)\n",
    "        \n",
    "    elif 19 < standoff_distance <= 25:\n",
    "        d_set[:,0:3] = sc5.transform(d_set[:,0:3])\n",
    "        p_f = ann5.predict(d_set)\n",
    "        \n",
    "    elif 25 < standoff_distance <= 30:\n",
    "        d_set[:,0:3] = sc6.transform(d_set[:,0:3])\n",
    "        p_f = ann6.predict(d_set)\n",
    "        \n",
    "    else:\n",
    "        print (\"Out of trained range\")\n",
    "    return p_f\n",
    "\n",
    "def p_fluctuation2(d_set):\n",
    "    d_set[:,0:4] = sc2.transform(d_set[:,0:4])\n",
    "    p_f = ann2.predict(d_set)\n",
    "    \n",
    "    return p_f\n",
    "\n",
    "if termination_time < arrival_time:\n",
    "    start = 0\n",
    "    end = termination_time\n",
    "    values1 = stable_state(start, end, interval)\n",
    "    arrival_time = 0\n",
    "    end_time = 0\n",
    "\n",
    "else:\n",
    "    start = 0\n",
    "    end = arrival_time\n",
    "    values1 = stable_state(start, end, interval)\n",
    "    \n",
    "    if termination_time < end_time:\n",
    "        end_time = termination_time\n",
    "        \n",
    "    else:\n",
    "        start = end_time\n",
    "        end = termination_time\n",
    "        values3 = stable_state(start, end, interval)\n",
    "\n",
    "t2= np.arange(arrival_time, end_time + interval, interval)\n",
    "t2 = t2.reshape(len(t2),1)\n",
    "\n",
    "if t2[0] < change_time:\n",
    "    ph2 = np.array([[0, 1]])\n",
    "else:\n",
    "    ph2 = np.array([[1, 0]])\n",
    "    \n",
    "positive = np.array([0, 1])\n",
    "negative= np.array([1, 0])\n",
    "\n",
    "for time in t2[1:]:\n",
    "    if time < change_time:\n",
    "        ph2=np.vstack((ph2,positive))\n",
    "    else:\n",
    "        ph2=np.vstack((ph2,negative))\n",
    "        \n",
    "m2 = np.repeat(Mass, len(t2)).reshape(len(t2),1)\n",
    "d2 = np.repeat(Standoff_distance, len(t2)).reshape(len(t2),1)\n",
    "a2 = np.repeat(Angle, len(t2)).reshape(len(t2),1)\n",
    "d_set = np.concatenate ((m2, d2, a2, t2, ph2), axis = 1)\n",
    "\n",
    "#p2 = p_fluctuation(Standoff_distance, d_set)\n",
    "p2 = p_fluctuation2(d_set)\n",
    "p2 = p2.reshape((-1, 1))\n",
    "values2 = np.concatenate ((t2, p2), axis = 1)\n",
    "\n",
    "\n",
    "if 'values2' in globals():\n",
    "    ml_model = np.concatenate((values1, values2), axis=0)\n",
    "    if 'values3' in globals():\n",
    "        ml_model = np.concatenate((ml_model, values3), axis=0)\n",
    "else:\n",
    "    ml_model = values1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11caee01fcf75405"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_path = Path('G:/Chamodi/LSDYNA3D/RP_TH_models/RP_Comparison_with_Numerical/4.5m17.3kg/2')\n",
    "lsdyna = pd.read_csv(file_path, delimiter=\"\\s+\", skiprows = 8, skipfooter = 1, header = None, names = [\"Time\", \"Pressure\"])\n",
    "\n",
    "Time_adj = 2\n",
    "Pressure_adj = 101.25\n",
    "\n",
    "lsdyna['Time'] = (lsdyna['Time']*1000)+ Time_adj\n",
    "lsdyna['Pressure'] = (lsdyna['Pressure']/1000)- Pressure_adj"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cbc49728bb5183d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file_path = Path('G:/Chamodi/LSDYNA3D/RP_TH_models/RP_Comparison_with_Numerical/2.2m15.8kg/8.xlsx')\n",
    "lsdyna = pd.read_excel(file_path, names = [\"Time\", \"Pressure\"])\n",
    "\n",
    "lsdyna['Time'] = (lsdyna['Time']*1000)\n",
    "lsdyna['Pressure'] = (lsdyna['Pressure']/1000)\n",
    "\n",
    "X2 = lsdyna['Time']\n",
    "y2 = lsdyna['Pressure']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97458bdfdcf98b66"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X2 = lsdyna['Time']\n",
    "pressure = lsdyna['Pressure']\n",
    "\n",
    "\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Identify peaks with tolerance parameters\n",
    "height_threshold = 3  # Minimum height of peaks to detect\n",
    "prominence = 0.01  # Minimum prominence of peaks in pressure units\n",
    "width = 6  # Minimum width of peaks in time units\n",
    "\n",
    "# Find peaks\n",
    "peaks, properties = find_peaks(pressure, height=height_threshold, prominence=prominence, width=width)\n",
    "print (peaks)\n",
    "\n",
    "# Ensure there are at least two peaks\n",
    "if len(peaks) >= 1:\n",
    "    #first_peak_index = peaks[0]\n",
    "\n",
    "    # Create a copy of pressure data to modify\n",
    "    smoothed_pressure = np.copy(pressure)\n",
    "\n",
    "    # Define a window around each peak after the first peak to apply smoothing\n",
    "    window_radius_r = 60\n",
    "    window_radius_l = 30 # Adjust the window radius as needed\n",
    "\n",
    "    for peak_index in peaks[1:]:\n",
    "        start = max(peak_index - window_radius_l, 0)\n",
    "        end = min(peak_index + window_radius_r + 1, len(pressure))\n",
    "\n",
    "        # Extract values just outside the window\n",
    "        if start > 0 and end < len(pressure):\n",
    "            start_value = pressure[start - 1]\n",
    "            end_value = pressure[end]\n",
    "        elif start > 0:\n",
    "            start_value = pressure[start - 1]\n",
    "            end_value = start_value\n",
    "        elif end < len(pressure):\n",
    "            end_value = pressure[end]\n",
    "            start_value = end_value\n",
    "        else:\n",
    "            start_value = end_value = np.mean(pressure)  # Fallback in case there are no valid points\n",
    "\n",
    "        # Create a linearly spaced array between start_value and end_value\n",
    "        interpolated_values = np.linspace(start_value, end_value, end - start)\n",
    "\n",
    "        # Apply the interpolated values to the points within the window\n",
    "        smoothed_pressure[start:end] = interpolated_values\n",
    "        \n",
    "y2 = smoothed_pressure\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(lsdyna['Time'], lsdyna['Pressure'], color = 'blue',label = 'Numerical')\n",
    "plt.plot(lsdyna['Time'], y2, color = 'red',label = 'Numerical')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97799b4f82c799bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X1 = ml_model[:,0]\n",
    "y1 = ml_model[:,1]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.plot(X1, y1, linestyle = 'dashed', color = 'red', label = 'ANN framework')\n",
    "plt.plot(X2, y2, color = 'blue',label = 'Numerical')\n",
    "plt.ylabel('Reflected overpressure (kPa)',fontsize = 20)\n",
    "plt.xlabel('Time (ms)',fontsize = 20)\n",
    "plt.xticks(fontsize = 18)\n",
    "plt.yticks(fontsize = 18)\n",
    "plt.legend(fontsize = 20)\n",
    "\n",
    "# Create the annotation text\n",
    "annotation_text = (f'Peak reflected overpressure\\n\\nANN framework - {round(max(y1), 2)} kPa\\n'\n",
    "                   f'Numerical - {round(max(y2), 2)} kPa')\n",
    "\n",
    "plt.annotate(annotation_text,xy=(0.4,0.5), xycoords='figure fraction', fontsize=18)\n",
    "plt.xlim (2, termination_time)\n",
    "plt.savefig('RP_Num&ANN_Comparison/Section2_4.5m17.3kg/5deg1.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37577a67aed611ba"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
