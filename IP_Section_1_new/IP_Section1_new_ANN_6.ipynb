{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3cbb7fRy-eyr"
   },
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8sNDnxE2-pwE"
   },
   "source": [
    "## Part 1 - Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lxChR1Rk-umf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "from pathlib import Path\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AG3FQEch-yuA"
   },
   "source": [
    "## Part 2 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access input folder\n",
    "current_dir = Path.cwd()\n",
    "parent_dir = current_dir.parent\n",
    "input_dir1 = Path (\"G:/Chamodi/LSDYNA_1D/Incident_pressure_time_history/Near_field_time_history_dataset\")\n",
    "print (\"input\",input_dir1)\n",
    "\n",
    "#Access folders inside input folder\n",
    "input_dir2_train =  [folder_input for folder_input in input_dir1.iterdir() if folder_input.is_dir()]\n",
    "print (\"2\",input_dir2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access each folder inside input folder\n",
    "dataset_train = pd.DataFrame(columns = ['Mass', 'Standoff distance', 'Time', 'Phase', 'Pressure'])\n",
    "df_list_train = []\n",
    "for folder_train in input_dir2_train:\n",
    "    \n",
    "    # Make a list of data file names\n",
    "    files_train = list(file_train for file_train in folder_train.rglob(\"*.xlsx\") if 5 <= int(file_train.stem))\n",
    "    print (\"folder\", folder_train)\n",
    "    \n",
    "    for file_train in files_train:\n",
    "        df_list_train.append(pd.read_excel(file_train))\n",
    "        print (file_train.stem)\n",
    "\n",
    "dataset_train = pd.concat(df_list_train, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B9CV13Co_HHM"
   },
   "outputs": [],
   "source": [
    "dataset_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_train = pd.get_dummies(dataset_train, columns =['Phase'], dtype = np.uint8)\n",
    "dataset_train.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_train.head(30)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset_train['Pressure']\n",
    "X = dataset_train.drop(['Pressure'], axis=1)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy array\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L5edeb2r_agx"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.4,\n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L5edeb2r_agx"
   },
   "outputs": [],
   "source": [
    "X_val, X_test, y_val, y_test = train_test_split(X_test,\n",
    "                                                y_test,\n",
    "                                                test_size = 0.5,\n",
    "                                                random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train[:,0:3] = sc.fit_transform(X_train[:, 0:3])\n",
    "print (X_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_test[:,0:3] = sc.transform(X_test[:,0:3])\n",
    "print (X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_val[:,0:3] = sc.transform(X_val[:,0:3])\n",
    "print (X_val)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print (\"X_train\", len(X_train))\n",
    "print (\"X_test\", len(X_test))\n",
    "print (\"X_val\", len(X_val))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_mSLlAT9_eyI"
   },
   "source": [
    "## Part 3 - Building the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ann = tf.keras.models.Sequential()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ksO_Vv40AHix"
   },
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=150,\n",
    "                              input_shape=(X_train.shape[1],),\n",
    "                              activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=125,\n",
    "                              activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=100,\n",
    "                              activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=75,\n",
    "                              activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFATpzsUAkLL"
   },
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=1,\n",
    "                              activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fq7e4fF6A1yy"
   },
   "source": [
    "## Part 4 - Training the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pesgbWlCAtB4"
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.optimizers import Adam\n",
    "from keras.optimizers import Adam\n",
    "opt = Adam(learning_rate=0.001)\n",
    "ann.compile(optimizer = opt,\n",
    "            loss = 'mean_squared_error',\n",
    "            metrics = ['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# protects from unnecessary further training of the model if a particular metric does not continue to improve over a number of n epochs. In such a case, the model training would be automatically aborted.\n",
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss',\n",
    "                   mode='min',\n",
    "                   patience=50,\n",
    "                   restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "c_vV-tiiA5zn",
    "outputId": "4a2b6ee6-ed75-4698-9069-b250e613803f"
   },
   "outputs": [],
   "source": [
    "history = ann.fit(X_train, y_train,\n",
    "                    validation_data = (X_val, y_val),\n",
    "                    callbacks=[es],\n",
    "                    epochs=500,\n",
    "                    batch_size=30,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see the training and validation accuracy by epoch\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss'] # you can change this\n",
    "val_loss_values = history_dict['val_loss'] # you can also change this\n",
    "epochs = range(1, len(loss_values) + 1) # range of X (no. of epochs)\n",
    "\n",
    "# Set global font to Times New Roman and font size\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "# Create a plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(epochs, loss_values, 'blue', label='Train set')\n",
    "plt.plot(epochs, val_loss_values, 'orange', label='Validation set')\n",
    "#plt.title('Training and testing loss')\n",
    "\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.savefig('Section1_ANN3_2.png', dpi=200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values1 = history_dict['mae'] # you can change this\n",
    "val_loss_values1 = history_dict['val_mae'] # you can also change this\n",
    "epochs = range(1, len(loss_values1) + 1) # range of X (no. of epochs)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(epochs, loss_values1, 'blue', label='Train set MAE')\n",
    "plt.plot(epochs, val_loss_values1, 'orange', label='Validation set MAE')\n",
    "#plt.title('Training and testing MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.savefig('Section1_ANN3_3.png', dpi=200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.save('Section1_ANN_3')\n",
    "import pickle\n",
    "pickle.dump(sc, open('Section1_scaler_ANN_3.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot of actual vs. pred\n",
    "# specify the dimensions \n",
    "fig, axes = plt.subplots(1,2) # 1 row, 2 columns\n",
    "\n",
    "# this makes the individual subplots\n",
    "# Training Results\n",
    "axes[0].scatter(x=y_train, y=ann.predict(X_train)) #first row, first entry (left top)\n",
    "axes[0].set_xlabel(\"Actual\", fontsize=10)\n",
    "axes[0].set_ylabel(\"Predicted\",  fontsize=10)\n",
    "axes[0].set_title(\"Training\")\n",
    "# add 45 deg line\n",
    "x = np.linspace(*axes[0].get_xlim())\n",
    "axes[0].plot(x, x, color='red')\n",
    "# Validation Results\n",
    "axes[1].scatter(x=y_val, y=ann.predict(X_val)) # first row, second entry (right top)\n",
    "axes[1].set_xlabel(\"Actual\", fontsize=10)\n",
    "axes[1].set_ylabel(\"Predicted\",  fontsize=10)\n",
    "axes[1].set_title(\"Validation\")\n",
    "# add 45 deg line\n",
    "x = np.linspace(*axes[1].get_xlim())\n",
    "axes[1].plot(x, x, color='red')\n",
    "\n",
    "# tight layout\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('Section1_ANN3_1.png', dpi=200, bbox_inches='tight')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "IA0yApEmBG1X",
    "outputId": "cb981e1f-9204-4a2a-fece-9d66a6919189"
   },
   "outputs": [],
   "source": [
    "y_pred = ann.predict(X_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(y_test, y_pred, color=\"blue\")\n",
    "plt.xlabel ('Actual data')\n",
    "plt.ylabel ('Predicted data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "# Initialize layout\n",
    "fig1, ax1 = plt.subplots(figsize = (6, 6))\n",
    "\n",
    "# Set global font to Times New Roman and font size\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "b_l = -200\n",
    "u_l = 4500\n",
    "\n",
    "# Add scatterplot\n",
    "ax1.scatter(y_test, y_pred, s=70, alpha=1, edgecolors=\"k\",c='mediumblue',zorder=5, label = 'Points')\n",
    "\n",
    "\n",
    "plt.ylabel('Predicted Incident Overpressure (kPa)')\n",
    "plt.xlabel('Target Incident Overpressure (kPa)')\n",
    "plt.xticks(fontsize = 19)\n",
    "plt.yticks(fontsize = 19)\n",
    "\n",
    "\n",
    "x1 = np.linspace(b_l, u_l)\n",
    "\n",
    "plt.plot(x1, x1, 'Red', label='45\\N{DEGREE SIGN} line',lw=2.5,alpha=1)\n",
    "\n",
    "plt.ylim(b_l, u_l)\n",
    "plt.xlim(b_l, u_l)\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=4, frameon = False, fontsize = 20)\n",
    "\n",
    "#plt.grid()\n",
    "\n",
    "ax1.spines['left'].set_color('black')        # setting up Y-axis tick color to red\n",
    "ax1.spines['bottom'].set_color('black')         #setting up above X-axis tick color to red\n",
    "\n",
    "plt.savefig('Section1_ANN3.png', dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "trainpreds = ann.predict(X_train)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(mean_absolute_error(y_train, trainpreds)) # train\n",
    "print(mean_absolute_error(y_test, y_pred)) # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print (r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_train = r2_score(y_train, trainpreds)\n",
    "print (r2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count0 = 0\n",
    "for i in range(len(y_pred)):\n",
    "    line1 = y_pred[i] - 0.95*y_test[i]\n",
    "    line2 = y_pred[i] - 1.05*y_test[i]\n",
    "    mask0 = (line1 > 0) & (line2 < 0)\n",
    "    count0 = np.sum(mask0+count0)\n",
    "\n",
    "count1 = 0\n",
    "for i in range(len(y_pred)):\n",
    "    line1 = y_pred[i] - 0.9*y_test[i]\n",
    "    line2 = y_pred[i] - 1.1*y_test[i]\n",
    "    mask1 = (line1 > 0) & (line2 < 0)\n",
    "    count1 = np.sum(mask1+count1)\n",
    "\n",
    "count2 = 0\n",
    "for j in range(len(y_pred)):\n",
    "    line3 = y_pred[j] - 0.8*y_test[j]\n",
    "    line4 = y_pred[j] - 1.2*y_test[j]\n",
    "    mask2 = (line3 > 0) & (line4 < 0)\n",
    "    count2 = np.sum(mask2+count2)\n",
    "\n",
    "\n",
    "count3 = 0    \n",
    "for k in range(len(y_pred)):\n",
    "    line5 = y_pred[k] - 0.8*y_test[k]\n",
    "    line6 = y_pred[k] - 1.2*y_test[k]\n",
    "    mask3 = (line5 < 0) or (line6 > 0)\n",
    "    count3 = np.sum(mask3+count3)\n",
    "   \n",
    "\n",
    "print ('Within 5% margin', format((count0/len (y_pred)),'.2%'))\n",
    "print ('Within 10% margin', format((count1/len (y_pred)),'.2%'))\n",
    "print ('Within 20% margin', format((count2/len (y_pred)),'.2%'))\n",
    "print ('Out of 20% margin', format((count3/len (y_pred)),'.2%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_r2 = 1 - ((1 - r2) * (len(X_test) - 1) / (len(X_test) - len (X_test[0]) - 1))\n",
    "print (adj_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "math.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Artificial Neural Network",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
