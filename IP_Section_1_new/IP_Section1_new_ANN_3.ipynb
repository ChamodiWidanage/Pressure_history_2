{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3cbb7fRy-eyr"
   },
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8sNDnxE2-pwE"
   },
   "source": [
    "## Part 1 - Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lxChR1Rk-umf",
    "ExecuteTime": {
     "end_time": "2024-08-21T06:05:03.880708900Z",
     "start_time": "2024-08-21T06:04:55.130386100Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "from pathlib import Path\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AG3FQEch-yuA"
   },
   "source": [
    "## Part 2 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T06:05:03.881708300Z",
     "start_time": "2024-08-21T06:05:03.841639500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input G:\\Chamodi\\LSDYNA_1D\\Incident_pressure_time_history\\Near_field_time_history_dataset\n",
      "2 [WindowsPath('G:/Chamodi/LSDYNA_1D/Incident_pressure_time_history/Near_field_time_history_dataset/L13_1'), WindowsPath('G:/Chamodi/LSDYNA_1D/Incident_pressure_time_history/Near_field_time_history_dataset/L13_13'), WindowsPath('G:/Chamodi/LSDYNA_1D/Incident_pressure_time_history/Near_field_time_history_dataset/L13_17'), WindowsPath('G:/Chamodi/LSDYNA_1D/Incident_pressure_time_history/Near_field_time_history_dataset/L13_21'), WindowsPath('G:/Chamodi/LSDYNA_1D/Incident_pressure_time_history/Near_field_time_history_dataset/L13_25'), WindowsPath('G:/Chamodi/LSDYNA_1D/Incident_pressure_time_history/Near_field_time_history_dataset/L13_29'), WindowsPath('G:/Chamodi/LSDYNA_1D/Incident_pressure_time_history/Near_field_time_history_dataset/L13_33'), WindowsPath('G:/Chamodi/LSDYNA_1D/Incident_pressure_time_history/Near_field_time_history_dataset/L13_37'), WindowsPath('G:/Chamodi/LSDYNA_1D/Incident_pressure_time_history/Near_field_time_history_dataset/L13_41'), WindowsPath('G:/Chamodi/LSDYNA_1D/Incident_pressure_time_history/Near_field_time_history_dataset/L13_45'), WindowsPath('G:/Chamodi/LSDYNA_1D/Incident_pressure_time_history/Near_field_time_history_dataset/L13_49'), WindowsPath('G:/Chamodi/LSDYNA_1D/Incident_pressure_time_history/Near_field_time_history_dataset/L13_5'), WindowsPath('G:/Chamodi/LSDYNA_1D/Incident_pressure_time_history/Near_field_time_history_dataset/L13_53'), WindowsPath('G:/Chamodi/LSDYNA_1D/Incident_pressure_time_history/Near_field_time_history_dataset/L13_57'), WindowsPath('G:/Chamodi/LSDYNA_1D/Incident_pressure_time_history/Near_field_time_history_dataset/L13_61'), WindowsPath('G:/Chamodi/LSDYNA_1D/Incident_pressure_time_history/Near_field_time_history_dataset/L13_65'), WindowsPath('G:/Chamodi/LSDYNA_1D/Incident_pressure_time_history/Near_field_time_history_dataset/L13_69'), WindowsPath('G:/Chamodi/LSDYNA_1D/Incident_pressure_time_history/Near_field_time_history_dataset/L13_70'), WindowsPath('G:/Chamodi/LSDYNA_1D/Incident_pressure_time_history/Near_field_time_history_dataset/L13_9')]\n"
     ]
    }
   ],
   "source": [
    "#Access input folder\n",
    "current_dir = Path.cwd()\n",
    "parent_dir = current_dir.parent\n",
    "input_dir1 = Path (\"G:/Chamodi/LSDYNA_1D/Incident_pressure_time_history/Near_field_time_history_dataset\")\n",
    "print (\"input\",input_dir1)\n",
    "\n",
    "#Access folders inside input folder\n",
    "input_dir2_train =  [folder_input for folder_input in input_dir1.iterdir() if folder_input.is_dir()]\n",
    "print (\"2\",input_dir2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T06:05:16.328908300Z",
     "start_time": "2024-08-21T06:05:03.867165200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder G:\\Chamodi\\LSDYNA_1D\\Incident_pressure_time_history\\Near_field_time_history_dataset\\L13_1\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "5\n",
      "7\n",
      "9\n",
      "folder G:\\Chamodi\\LSDYNA_1D\\Incident_pressure_time_history\\Near_field_time_history_dataset\\L13_13\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "5\n",
      "7\n",
      "9\n",
      "folder G:\\Chamodi\\LSDYNA_1D\\Incident_pressure_time_history\\Near_field_time_history_dataset\\L13_17\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "5\n",
      "7\n",
      "9\n",
      "folder G:\\Chamodi\\LSDYNA_1D\\Incident_pressure_time_history\\Near_field_time_history_dataset\\L13_21\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "5\n",
      "7\n",
      "9\n",
      "folder G:\\Chamodi\\LSDYNA_1D\\Incident_pressure_time_history\\Near_field_time_history_dataset\\L13_25\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "5\n",
      "7\n",
      "9\n",
      "folder G:\\Chamodi\\LSDYNA_1D\\Incident_pressure_time_history\\Near_field_time_history_dataset\\L13_29\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "5\n",
      "7\n",
      "9\n",
      "folder G:\\Chamodi\\LSDYNA_1D\\Incident_pressure_time_history\\Near_field_time_history_dataset\\L13_33\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "5\n",
      "7\n",
      "9\n",
      "folder G:\\Chamodi\\LSDYNA_1D\\Incident_pressure_time_history\\Near_field_time_history_dataset\\L13_37\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "5\n",
      "7\n",
      "9\n",
      "folder G:\\Chamodi\\LSDYNA_1D\\Incident_pressure_time_history\\Near_field_time_history_dataset\\L13_41\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "5\n",
      "7\n",
      "9\n",
      "folder G:\\Chamodi\\LSDYNA_1D\\Incident_pressure_time_history\\Near_field_time_history_dataset\\L13_45\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "5\n",
      "7\n",
      "9\n",
      "folder G:\\Chamodi\\LSDYNA_1D\\Incident_pressure_time_history\\Near_field_time_history_dataset\\L13_49\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "5\n",
      "7\n",
      "9\n",
      "folder G:\\Chamodi\\LSDYNA_1D\\Incident_pressure_time_history\\Near_field_time_history_dataset\\L13_5\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "5\n",
      "7\n",
      "9\n",
      "folder G:\\Chamodi\\LSDYNA_1D\\Incident_pressure_time_history\\Near_field_time_history_dataset\\L13_53\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "5\n",
      "7\n",
      "9\n",
      "folder G:\\Chamodi\\LSDYNA_1D\\Incident_pressure_time_history\\Near_field_time_history_dataset\\L13_57\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "5\n",
      "7\n",
      "9\n",
      "folder G:\\Chamodi\\LSDYNA_1D\\Incident_pressure_time_history\\Near_field_time_history_dataset\\L13_61\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "5\n",
      "7\n",
      "9\n",
      "folder G:\\Chamodi\\LSDYNA_1D\\Incident_pressure_time_history\\Near_field_time_history_dataset\\L13_65\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "5\n",
      "7\n",
      "9\n",
      "folder G:\\Chamodi\\LSDYNA_1D\\Incident_pressure_time_history\\Near_field_time_history_dataset\\L13_69\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "5\n",
      "7\n",
      "9\n",
      "folder G:\\Chamodi\\LSDYNA_1D\\Incident_pressure_time_history\\Near_field_time_history_dataset\\L13_70\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "5\n",
      "7\n",
      "9\n",
      "folder G:\\Chamodi\\LSDYNA_1D\\Incident_pressure_time_history\\Near_field_time_history_dataset\\L13_9\n",
      "11\n",
      "13\n",
      "15\n",
      "17\n",
      "5\n",
      "7\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Access each folder inside input folder\n",
    "dataset_train = pd.DataFrame(columns = ['Mass', 'Standoff distance', 'Time', 'Phase', 'Pressure'])\n",
    "df_list_train = []\n",
    "for folder_train in input_dir2_train:\n",
    "    \n",
    "    # Make a list of data file names\n",
    "    files_train = list(file_train for file_train in folder_train.rglob(\"*.xlsx\") if 5 <= int(file_train.stem))\n",
    "    print (\"folder\", folder_train)\n",
    "    \n",
    "    for file_train in files_train:\n",
    "        df_list_train.append(pd.read_excel(file_train))\n",
    "        print (file_train.stem)\n",
    "\n",
    "dataset_train = pd.concat(df_list_train, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B9CV13Co_HHM",
    "ExecuteTime": {
     "end_time": "2024-08-21T06:05:16.328908300Z",
     "start_time": "2024-08-21T06:05:13.023400400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 36100 entries, 0 to 36099\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Mass               36100 non-null  float64\n",
      " 1   Standoff distance  36100 non-null  float64\n",
      " 2   Time               36100 non-null  float64\n",
      " 3   Phase              36100 non-null  object \n",
      " 4   Pressure           36100 non-null  float64\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 36100 entries, 0 to 36099\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Mass               36100 non-null  float64\n",
      " 1   Standoff distance  36100 non-null  float64\n",
      " 2   Time               36100 non-null  float64\n",
      " 3   Pressure           36100 non-null  float64\n",
      " 4   Phase_negative     36100 non-null  uint8  \n",
      " 5   Phase_positive     36100 non-null  uint8  \n",
      "dtypes: float64(4), uint8(2)\n",
      "memory usage: 1.2 MB\n"
     ]
    }
   ],
   "source": [
    "dataset_train = pd.get_dummies(dataset_train, columns =['Phase'], dtype = np.uint8)\n",
    "dataset_train.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-21T06:05:16.329907600Z",
     "start_time": "2024-08-21T06:05:13.057928600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "    Mass  Standoff distance    Time   Pressure  Phase_negative  Phase_positive\n0    0.5               2.25  3.0700  77.214000               0               1\n1    0.5               2.25  3.1497  68.664000               0               1\n2    0.5               2.25  3.2096  62.758000               0               1\n3    0.5               2.25  3.2800  56.423000               0               1\n4    0.5               2.25  3.3497  50.700000               0               1\n5    0.5               2.25  3.4299  44.696000               0               1\n6    0.5               2.25  3.5000  39.944000               0               1\n7    0.5               2.25  3.5399  37.407000               0               1\n8    0.5               2.25  3.5898  34.401000               0               1\n9    0.5               2.25  3.6400  31.563000               0               1\n10   0.5               2.25  3.6798  29.414000               0               1\n11   0.5               2.25  3.7198  27.368000               0               1\n12   0.5               2.25  3.7698  24.927000               0               1\n13   0.5               2.25  3.8599  20.866000               0               1\n14   0.5               2.25  3.9100  18.784000               0               1\n15   0.5               2.25  3.9598  16.818000               0               1\n16   0.5               2.25  4.0197  14.575000               0               1\n17   0.5               2.25  4.0697  12.829000               0               1\n18   0.5               2.25  4.1299  10.845000               0               1\n19   0.5               2.25  4.1898   8.977000               0               1\n20   0.5               2.25  4.2298   7.779000               0               1\n21   0.5               2.25  4.2698   6.662000               0               1\n22   0.5               2.25  4.3597   4.284000               0               1\n23   0.5               2.25  4.3998   3.294000               0               1\n24   0.5               2.25  4.4698   1.647000               0               1\n25   0.5               2.25  4.5399   0.108000               0               1\n26   0.5               2.25  4.5497  -0.099000               1               0\n27   0.5               2.25  4.7498  -3.878398               1               0\n28   0.5               2.25  4.9397  -6.831711               1               0\n29   0.5               2.25  5.1600  -9.628898               1               0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Mass</th>\n      <th>Standoff distance</th>\n      <th>Time</th>\n      <th>Pressure</th>\n      <th>Phase_negative</th>\n      <th>Phase_positive</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>3.0700</td>\n      <td>77.214000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>3.1497</td>\n      <td>68.664000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>3.2096</td>\n      <td>62.758000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>3.2800</td>\n      <td>56.423000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>3.3497</td>\n      <td>50.700000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>3.4299</td>\n      <td>44.696000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>3.5000</td>\n      <td>39.944000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>3.5399</td>\n      <td>37.407000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>3.5898</td>\n      <td>34.401000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>3.6400</td>\n      <td>31.563000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>3.6798</td>\n      <td>29.414000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>3.7198</td>\n      <td>27.368000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>3.7698</td>\n      <td>24.927000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>3.8599</td>\n      <td>20.866000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>3.9100</td>\n      <td>18.784000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>3.9598</td>\n      <td>16.818000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>4.0197</td>\n      <td>14.575000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>4.0697</td>\n      <td>12.829000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>4.1299</td>\n      <td>10.845000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>4.1898</td>\n      <td>8.977000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>4.2298</td>\n      <td>7.779000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>4.2698</td>\n      <td>6.662000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>4.3597</td>\n      <td>4.284000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>4.3998</td>\n      <td>3.294000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>4.4698</td>\n      <td>1.647000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>4.5399</td>\n      <td>0.108000</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>4.5497</td>\n      <td>-0.099000</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>4.7498</td>\n      <td>-3.878398</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>4.9397</td>\n      <td>-6.831711</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.5</td>\n      <td>2.25</td>\n      <td>5.1600</td>\n      <td>-9.628898</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.head(30)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-21T06:05:16.330906500Z",
     "start_time": "2024-08-21T06:05:13.204547400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T06:05:16.331915300Z",
     "start_time": "2024-08-21T06:05:13.363198300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36100, 5) (36100,)\n"
     ]
    }
   ],
   "source": [
    "y = dataset_train['Pressure']\n",
    "X = dataset_train.drop(['Pressure'], axis=1)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T06:05:16.331915300Z",
     "start_time": "2024-08-21T06:05:13.517817400Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert to numpy array\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L5edeb2r_agx",
    "ExecuteTime": {
     "end_time": "2024-08-21T06:05:16.333914300Z",
     "start_time": "2024-08-21T06:05:13.694981900Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.4,\n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L5edeb2r_agx",
    "ExecuteTime": {
     "end_time": "2024-08-21T06:05:16.333914300Z",
     "start_time": "2024-08-21T06:05:15.085762700Z"
    }
   },
   "outputs": [],
   "source": [
    "X_val, X_test, y_val, y_test = train_test_split(X_test,\n",
    "                                                y_test,\n",
    "                                                test_size = 0.5,\n",
    "                                                random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.06952585  0.94780125 -1.14909443  1.          0.        ]\n",
      " [ 0.12815462  0.49225093  1.73902461  1.          0.        ]\n",
      " [-1.65096961 -0.87440002 -1.39617281  0.          1.        ]\n",
      " ...\n",
      " [ 1.31423745  0.03670061  0.71465319  1.          0.        ]\n",
      " [ 0.52351556  0.94780125 -0.62636703  1.          0.        ]\n",
      " [-1.25560867  0.94780125 -1.16152346  1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train[:,0:3] = sc.fit_transform(X_train[:, 0:3])\n",
    "print (X_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-21T06:05:16.333914300Z",
     "start_time": "2024-08-21T06:05:15.100283500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.46488679 -1.32995034  0.23785481  1.          0.        ]\n",
      " [ 0.12815462  0.94780125  1.16165156  1.          0.        ]\n",
      " [ 0.52351556  0.03670061 -1.46075166  0.          1.        ]\n",
      " ...\n",
      " [ 1.31423745 -0.41884971 -1.20248591  1.          0.        ]\n",
      " [ 0.32583509 -1.32995034  0.42658741  1.          0.        ]\n",
      " [-0.66256726  0.94780125 -0.21907694  1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "X_test[:,0:3] = sc.transform(X_test[:,0:3])\n",
    "print (X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-21T06:05:16.334914900Z",
     "start_time": "2024-08-21T06:05:15.253939700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.32583509  0.03670061  1.26967626  1.          0.        ]\n",
      " [-0.66256726 -0.41884971 -1.04354071  1.          0.        ]\n",
      " [ 0.72119603 -0.87440002 -0.4376343   1.          0.        ]\n",
      " ...\n",
      " [-0.26720632  1.40335157 -0.56552552  1.          0.        ]\n",
      " [ 0.72119603 -1.32995034  1.52173372  1.          0.        ]\n",
      " [-0.46488679 -1.32995034 -1.38131011  1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "X_val[:,0:3] = sc.transform(X_val[:,0:3])\n",
    "print (X_val)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-21T06:05:16.334914900Z",
     "start_time": "2024-08-21T06:05:15.406092600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train 21660\n",
      "X_test 7220\n",
      "X_val 7220\n"
     ]
    }
   ],
   "source": [
    "print (\"X_train\", len(X_train))\n",
    "print (\"X_test\", len(X_test))\n",
    "print (\"X_val\", len(X_val))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-21T06:05:16.334914900Z",
     "start_time": "2024-08-21T06:05:15.548677500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_mSLlAT9_eyI"
   },
   "source": [
    "## Part 3 - Building the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "ann = tf.keras.models.Sequential()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-21T06:05:16.334914900Z",
     "start_time": "2024-08-21T06:05:15.708337500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ksO_Vv40AHix",
    "ExecuteTime": {
     "end_time": "2024-08-21T06:05:16.336921300Z",
     "start_time": "2024-08-21T06:05:15.927013500Z"
    }
   },
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=150,\n",
    "                              input_shape=(X_train.shape[1],),\n",
    "                              activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=125,\n",
    "                              activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=100,\n",
    "                              activation='relu'))\n",
    "ann.add(tf.keras.layers.Dense(units=75,\n",
    "                              activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YFATpzsUAkLL",
    "ExecuteTime": {
     "end_time": "2024-08-21T06:05:16.336921300Z",
     "start_time": "2024-08-21T06:05:16.212774Z"
    }
   },
   "outputs": [],
   "source": [
    "ann.add(tf.keras.layers.Dense(units=1,\n",
    "                              activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T06:05:23.614170600Z",
     "start_time": "2024-08-21T06:05:16.258817800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 150)               900       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 125)               18875     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               12600     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 75)                7575      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 76        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,026\n",
      "Trainable params: 40,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ann.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fq7e4fF6A1yy"
   },
   "source": [
    "## Part 4 - Training the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pesgbWlCAtB4",
    "ExecuteTime": {
     "end_time": "2024-08-21T06:05:23.615170600Z",
     "start_time": "2024-08-21T06:05:16.407016500Z"
    }
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.optimizers import Adam\n",
    "from keras.optimizers import Adam\n",
    "opt = Adam(learning_rate=0.001)\n",
    "ann.compile(optimizer = opt,\n",
    "            loss = 'mean_squared_error',\n",
    "            metrics = ['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T06:05:23.615170600Z",
     "start_time": "2024-08-21T06:05:16.595218500Z"
    }
   },
   "outputs": [],
   "source": [
    "# protects from unnecessary further training of the model if a particular metric does not continue to improve over a number of n epochs. In such a case, the model training would be automatically aborted.\n",
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss',\n",
    "                   mode='min',\n",
    "                   patience=50,\n",
    "                   restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "c_vV-tiiA5zn",
    "outputId": "4a2b6ee6-ed75-4698-9069-b250e613803f",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-08-21T06:05:16.736921200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "722/722 [==============================] - 170s 234ms/step - loss: 27380.8398 - mae: 50.2269 - val_loss: 24715.2070 - val_mae: 48.6944\n",
      "Epoch 2/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 20677.4180 - mae: 43.7191 - val_loss: 21278.7051 - val_mae: 43.8554\n",
      "Epoch 3/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 19817.9746 - mae: 41.9437 - val_loss: 20353.8262 - val_mae: 44.7694\n",
      "Epoch 4/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 19399.4941 - mae: 41.1839 - val_loss: 20780.6797 - val_mae: 41.4007\n",
      "Epoch 5/500\n",
      "722/722 [==============================] - 5s 6ms/step - loss: 19125.7051 - mae: 40.3642 - val_loss: 19353.3828 - val_mae: 41.1745\n",
      "Epoch 6/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 18682.4961 - mae: 39.9204 - val_loss: 24318.1562 - val_mae: 41.7716\n",
      "Epoch 7/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 17999.2168 - mae: 38.3449 - val_loss: 17259.3223 - val_mae: 39.7943\n",
      "Epoch 8/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 16180.2959 - mae: 35.9779 - val_loss: 17208.9512 - val_mae: 35.3238\n",
      "Epoch 9/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 14257.6514 - mae: 32.4329 - val_loss: 12382.6279 - val_mae: 27.4036\n",
      "Epoch 10/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 12120.7939 - mae: 28.6905 - val_loss: 13466.6455 - val_mae: 27.7216\n",
      "Epoch 11/500\n",
      "722/722 [==============================] - 5s 6ms/step - loss: 10582.2363 - mae: 25.7200 - val_loss: 13727.3164 - val_mae: 27.8858\n",
      "Epoch 12/500\n",
      "722/722 [==============================] - 8s 11ms/step - loss: 9727.9619 - mae: 23.8193 - val_loss: 9823.4551 - val_mae: 24.6566\n",
      "Epoch 13/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 9383.9248 - mae: 23.4872 - val_loss: 8756.5127 - val_mae: 23.4468\n",
      "Epoch 14/500\n",
      "722/722 [==============================] - 7s 10ms/step - loss: 8927.2832 - mae: 22.2906 - val_loss: 7182.2100 - val_mae: 19.8143\n",
      "Epoch 15/500\n",
      "722/722 [==============================] - 5s 6ms/step - loss: 7207.8789 - mae: 19.5866 - val_loss: 6328.7993 - val_mae: 17.2763\n",
      "Epoch 16/500\n",
      "722/722 [==============================] - 7s 9ms/step - loss: 6825.2632 - mae: 18.5651 - val_loss: 5301.5425 - val_mae: 14.5432\n",
      "Epoch 17/500\n",
      "722/722 [==============================] - 8s 12ms/step - loss: 7296.2617 - mae: 18.8364 - val_loss: 9062.5332 - val_mae: 19.1723\n",
      "Epoch 18/500\n",
      "722/722 [==============================] - 7s 9ms/step - loss: 7219.0435 - mae: 18.6262 - val_loss: 4984.3491 - val_mae: 13.6148\n",
      "Epoch 19/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 6991.3989 - mae: 18.6519 - val_loss: 5159.7339 - val_mae: 16.8321\n",
      "Epoch 20/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 7469.6494 - mae: 18.3024 - val_loss: 4964.9229 - val_mae: 13.3268\n",
      "Epoch 21/500\n",
      "722/722 [==============================] - 8s 11ms/step - loss: 6610.4111 - mae: 17.2168 - val_loss: 10176.0664 - val_mae: 24.5882\n",
      "Epoch 22/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 6187.9141 - mae: 16.1997 - val_loss: 7095.2178 - val_mae: 15.0297\n",
      "Epoch 23/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 7263.0728 - mae: 18.8131 - val_loss: 5126.5601 - val_mae: 15.9658\n",
      "Epoch 24/500\n",
      "722/722 [==============================] - 8s 10ms/step - loss: 6508.7520 - mae: 17.0684 - val_loss: 4390.1743 - val_mae: 12.4937\n",
      "Epoch 25/500\n",
      "722/722 [==============================] - 10s 14ms/step - loss: 6469.7705 - mae: 17.1703 - val_loss: 8822.0654 - val_mae: 15.1345\n",
      "Epoch 26/500\n",
      "722/722 [==============================] - 5s 8ms/step - loss: 7141.0586 - mae: 18.0944 - val_loss: 4496.4492 - val_mae: 16.2793\n",
      "Epoch 27/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 6619.0532 - mae: 17.4404 - val_loss: 5393.9395 - val_mae: 19.5751\n",
      "Epoch 28/500\n",
      "722/722 [==============================] - 7s 10ms/step - loss: 6268.1582 - mae: 15.4302 - val_loss: 5030.2754 - val_mae: 13.3827\n",
      "Epoch 29/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 6023.1807 - mae: 15.4508 - val_loss: 7470.5757 - val_mae: 20.6691\n",
      "Epoch 30/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 5468.1040 - mae: 14.6697 - val_loss: 4214.5381 - val_mae: 15.1075\n",
      "Epoch 31/500\n",
      "722/722 [==============================] - 6s 9ms/step - loss: 5790.1963 - mae: 15.0739 - val_loss: 4759.0488 - val_mae: 12.3925\n",
      "Epoch 32/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 5420.2017 - mae: 14.3430 - val_loss: 8397.9541 - val_mae: 23.2663\n",
      "Epoch 33/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 5474.7529 - mae: 14.8374 - val_loss: 4794.9458 - val_mae: 12.5535\n",
      "Epoch 34/500\n",
      "722/722 [==============================] - 12s 17ms/step - loss: 6982.6284 - mae: 17.1934 - val_loss: 4278.8799 - val_mae: 12.3430\n",
      "Epoch 35/500\n",
      "722/722 [==============================] - 9s 13ms/step - loss: 5955.3369 - mae: 16.0093 - val_loss: 27519.3379 - val_mae: 34.8082\n",
      "Epoch 36/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 7303.2319 - mae: 17.0342 - val_loss: 6531.3975 - val_mae: 14.8538\n",
      "Epoch 37/500\n",
      "722/722 [==============================] - 8s 11ms/step - loss: 4681.4380 - mae: 13.4895 - val_loss: 2838.0095 - val_mae: 9.6498\n",
      "Epoch 38/500\n",
      "722/722 [==============================] - 7s 10ms/step - loss: 5363.9565 - mae: 14.6945 - val_loss: 7589.4336 - val_mae: 16.8154\n",
      "Epoch 39/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 5886.7314 - mae: 14.7817 - val_loss: 7011.2026 - val_mae: 17.4091\n",
      "Epoch 40/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 4862.5073 - mae: 14.0811 - val_loss: 3810.3230 - val_mae: 12.2214\n",
      "Epoch 41/500\n",
      "722/722 [==============================] - 6s 9ms/step - loss: 4999.5215 - mae: 13.7444 - val_loss: 3136.9546 - val_mae: 10.9529\n",
      "Epoch 42/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 4765.3013 - mae: 12.9382 - val_loss: 5439.9639 - val_mae: 12.3963\n",
      "Epoch 43/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 4748.6538 - mae: 13.2799 - val_loss: 5728.2949 - val_mae: 18.3820\n",
      "Epoch 44/500\n",
      "722/722 [==============================] - 8s 10ms/step - loss: 4356.7300 - mae: 12.0990 - val_loss: 3276.2893 - val_mae: 9.7197\n",
      "Epoch 45/500\n",
      "722/722 [==============================] - 8s 10ms/step - loss: 6020.7075 - mae: 14.5975 - val_loss: 10573.9170 - val_mae: 17.6554\n",
      "Epoch 46/500\n",
      "722/722 [==============================] - 10s 14ms/step - loss: 5853.0918 - mae: 14.4279 - val_loss: 4550.9019 - val_mae: 13.1900\n",
      "Epoch 47/500\n",
      "722/722 [==============================] - 7s 9ms/step - loss: 5535.7490 - mae: 14.2663 - val_loss: 10708.5859 - val_mae: 19.6348\n",
      "Epoch 48/500\n",
      "722/722 [==============================] - 5s 6ms/step - loss: 5759.9707 - mae: 14.3672 - val_loss: 7644.9370 - val_mae: 15.6070\n",
      "Epoch 49/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 3891.7329 - mae: 11.8781 - val_loss: 3160.0803 - val_mae: 12.9392\n",
      "Epoch 50/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 5378.1348 - mae: 14.7034 - val_loss: 3048.9502 - val_mae: 11.9116\n",
      "Epoch 51/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 5441.1299 - mae: 14.3862 - val_loss: 4591.6558 - val_mae: 13.2338\n",
      "Epoch 52/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 4503.7871 - mae: 12.8066 - val_loss: 5825.0796 - val_mae: 13.3685\n",
      "Epoch 53/500\n",
      "722/722 [==============================] - 7s 10ms/step - loss: 4144.1616 - mae: 12.3895 - val_loss: 11568.7959 - val_mae: 21.9801\n",
      "Epoch 54/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 5181.8398 - mae: 13.9101 - val_loss: 5399.9966 - val_mae: 14.9795\n",
      "Epoch 55/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 5620.7031 - mae: 13.9014 - val_loss: 2833.0002 - val_mae: 11.2464\n",
      "Epoch 56/500\n",
      "722/722 [==============================] - 6s 9ms/step - loss: 5984.6938 - mae: 14.8623 - val_loss: 3775.2400 - val_mae: 12.0243\n",
      "Epoch 57/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 5312.5308 - mae: 13.6372 - val_loss: 6512.7891 - val_mae: 19.0139\n",
      "Epoch 58/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 4757.9517 - mae: 13.2761 - val_loss: 3202.8958 - val_mae: 10.2446\n",
      "Epoch 59/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 5843.7192 - mae: 13.8526 - val_loss: 2889.8433 - val_mae: 10.2626\n",
      "Epoch 60/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 4379.6978 - mae: 12.3780 - val_loss: 4810.3833 - val_mae: 12.4231\n",
      "Epoch 61/500\n",
      "722/722 [==============================] - 8s 10ms/step - loss: 4943.9492 - mae: 13.1066 - val_loss: 8953.7793 - val_mae: 20.7669\n",
      "Epoch 62/500\n",
      "722/722 [==============================] - 8s 11ms/step - loss: 3642.4697 - mae: 11.1449 - val_loss: 3657.4197 - val_mae: 13.0926\n",
      "Epoch 63/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 3542.6013 - mae: 10.9304 - val_loss: 3628.9368 - val_mae: 10.4461\n",
      "Epoch 64/500\n",
      "722/722 [==============================] - 8s 11ms/step - loss: 4325.7969 - mae: 12.2807 - val_loss: 10735.5410 - val_mae: 18.8804\n",
      "Epoch 65/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 4077.5393 - mae: 12.1072 - val_loss: 3191.7463 - val_mae: 8.8683\n",
      "Epoch 66/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 5149.4805 - mae: 12.9448 - val_loss: 3217.5203 - val_mae: 9.5970\n",
      "Epoch 67/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 3307.6606 - mae: 11.1693 - val_loss: 3598.4238 - val_mae: 13.0028\n",
      "Epoch 68/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 5844.1660 - mae: 14.8862 - val_loss: 9312.9365 - val_mae: 19.6380\n",
      "Epoch 69/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 5588.5508 - mae: 14.0373 - val_loss: 2522.3301 - val_mae: 10.6144\n",
      "Epoch 70/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 8394.8857 - mae: 17.1463 - val_loss: 3648.0420 - val_mae: 12.4811\n",
      "Epoch 71/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 4430.4604 - mae: 12.9338 - val_loss: 6863.0200 - val_mae: 17.6601\n",
      "Epoch 72/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 4196.5938 - mae: 12.4176 - val_loss: 2823.1887 - val_mae: 11.1937\n",
      "Epoch 73/500\n",
      "722/722 [==============================] - 7s 10ms/step - loss: 2946.9177 - mae: 10.6072 - val_loss: 14967.7832 - val_mae: 21.7644\n",
      "Epoch 74/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 5121.2544 - mae: 13.7740 - val_loss: 2807.5408 - val_mae: 9.8289\n",
      "Epoch 75/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 3682.4182 - mae: 11.3950 - val_loss: 2298.0828 - val_mae: 10.3316\n",
      "Epoch 76/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 3939.9373 - mae: 11.9903 - val_loss: 9532.8574 - val_mae: 20.0287\n",
      "Epoch 77/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 3880.1370 - mae: 11.4126 - val_loss: 3791.6572 - val_mae: 12.1525\n",
      "Epoch 78/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 3849.7666 - mae: 11.3471 - val_loss: 15478.5527 - val_mae: 24.2753\n",
      "Epoch 79/500\n",
      "722/722 [==============================] - 6s 9ms/step - loss: 4779.6343 - mae: 12.6913 - val_loss: 3434.0344 - val_mae: 12.0543\n",
      "Epoch 80/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 4419.7920 - mae: 12.0918 - val_loss: 5286.3789 - val_mae: 18.0106\n",
      "Epoch 81/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 3564.0894 - mae: 11.0528 - val_loss: 6258.4678 - val_mae: 17.6698\n",
      "Epoch 82/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 4512.4009 - mae: 12.3695 - val_loss: 10465.4375 - val_mae: 17.4671\n",
      "Epoch 83/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 4166.2754 - mae: 11.8519 - val_loss: 2510.8730 - val_mae: 10.0415\n",
      "Epoch 84/500\n",
      "722/722 [==============================] - 7s 9ms/step - loss: 4717.9219 - mae: 12.5116 - val_loss: 3403.0105 - val_mae: 15.4255\n",
      "Epoch 85/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 4848.6924 - mae: 13.0009 - val_loss: 3769.3958 - val_mae: 12.3118\n",
      "Epoch 86/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 4352.5815 - mae: 11.5294 - val_loss: 3398.3704 - val_mae: 10.8037\n",
      "Epoch 87/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 4176.7705 - mae: 11.8625 - val_loss: 3671.0842 - val_mae: 10.2992\n",
      "Epoch 88/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 4200.2422 - mae: 11.8066 - val_loss: 2716.5366 - val_mae: 10.5641\n",
      "Epoch 89/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 4215.9531 - mae: 11.7199 - val_loss: 4112.9014 - val_mae: 10.9042\n",
      "Epoch 90/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 4932.4883 - mae: 12.7134 - val_loss: 2716.7349 - val_mae: 10.9385\n",
      "Epoch 91/500\n",
      "722/722 [==============================] - 5s 6ms/step - loss: 5129.5542 - mae: 12.6987 - val_loss: 8065.3428 - val_mae: 20.9673\n",
      "Epoch 92/500\n",
      "722/722 [==============================] - 9s 12ms/step - loss: 3973.8381 - mae: 11.2979 - val_loss: 15249.4580 - val_mae: 29.7874\n",
      "Epoch 93/500\n",
      "722/722 [==============================] - 7s 9ms/step - loss: 4363.3047 - mae: 12.9425 - val_loss: 2835.8127 - val_mae: 9.7113\n",
      "Epoch 94/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 3313.6660 - mae: 11.0408 - val_loss: 3099.4729 - val_mae: 11.6023\n",
      "Epoch 95/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 3549.1741 - mae: 11.1274 - val_loss: 5757.6328 - val_mae: 19.1544\n",
      "Epoch 96/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 3098.7671 - mae: 10.6134 - val_loss: 4974.6431 - val_mae: 14.4601\n",
      "Epoch 97/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 4720.0659 - mae: 12.6557 - val_loss: 6568.2856 - val_mae: 16.0887\n",
      "Epoch 98/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 3975.8738 - mae: 11.2239 - val_loss: 2708.8301 - val_mae: 10.4807\n",
      "Epoch 99/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 4257.1143 - mae: 11.8395 - val_loss: 5313.8247 - val_mae: 14.1227\n",
      "Epoch 100/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 3912.5466 - mae: 11.2765 - val_loss: 2301.2832 - val_mae: 9.3304\n",
      "Epoch 101/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 3440.8491 - mae: 10.3043 - val_loss: 3051.5095 - val_mae: 10.4785\n",
      "Epoch 102/500\n",
      "722/722 [==============================] - 6s 9ms/step - loss: 3087.1792 - mae: 10.1945 - val_loss: 1992.8160 - val_mae: 8.2175\n",
      "Epoch 103/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 3674.2749 - mae: 10.6250 - val_loss: 3167.5598 - val_mae: 8.9455\n",
      "Epoch 104/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 4389.0234 - mae: 11.8823 - val_loss: 4371.0469 - val_mae: 11.6074\n",
      "Epoch 105/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 3101.8684 - mae: 9.9863 - val_loss: 20387.1074 - val_mae: 27.5068\n",
      "Epoch 106/500\n",
      "722/722 [==============================] - 6s 9ms/step - loss: 3750.2209 - mae: 10.8671 - val_loss: 3559.9719 - val_mae: 9.6246\n",
      "Epoch 107/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 3831.1064 - mae: 11.1956 - val_loss: 12560.3936 - val_mae: 21.0761\n",
      "Epoch 108/500\n",
      "722/722 [==============================] - 5s 6ms/step - loss: 3263.5232 - mae: 10.6172 - val_loss: 1847.1675 - val_mae: 9.5484\n",
      "Epoch 109/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 3451.8884 - mae: 10.1850 - val_loss: 4038.1692 - val_mae: 13.6128\n",
      "Epoch 110/500\n",
      "722/722 [==============================] - 5s 6ms/step - loss: 4423.4912 - mae: 11.5252 - val_loss: 1760.1829 - val_mae: 8.0233\n",
      "Epoch 111/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 4564.9404 - mae: 11.1834 - val_loss: 5474.9189 - val_mae: 16.6798\n",
      "Epoch 112/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 3310.2710 - mae: 10.5569 - val_loss: 1952.4348 - val_mae: 7.8461\n",
      "Epoch 113/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 4188.8447 - mae: 11.1878 - val_loss: 3013.1047 - val_mae: 10.2646\n",
      "Epoch 114/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 3121.0212 - mae: 10.4288 - val_loss: 3758.8582 - val_mae: 14.0760\n",
      "Epoch 115/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 3661.3833 - mae: 10.4390 - val_loss: 16341.6904 - val_mae: 23.6444\n",
      "Epoch 116/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 3721.3005 - mae: 11.2452 - val_loss: 1869.4238 - val_mae: 7.2768\n",
      "Epoch 117/500\n",
      "722/722 [==============================] - 5s 6ms/step - loss: 3013.0481 - mae: 9.1504 - val_loss: 21833.7402 - val_mae: 37.4897\n",
      "Epoch 118/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 3914.6213 - mae: 10.6225 - val_loss: 2944.5996 - val_mae: 8.7478\n",
      "Epoch 119/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 3825.0913 - mae: 10.7239 - val_loss: 6695.0547 - val_mae: 13.3267\n",
      "Epoch 120/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 4633.9067 - mae: 11.6989 - val_loss: 1818.3358 - val_mae: 7.5155\n",
      "Epoch 121/500\n",
      "722/722 [==============================] - 5s 6ms/step - loss: 3392.6807 - mae: 10.7382 - val_loss: 12603.7461 - val_mae: 21.2745\n",
      "Epoch 122/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 3932.8730 - mae: 11.0939 - val_loss: 2272.7502 - val_mae: 8.6971\n",
      "Epoch 123/500\n",
      "722/722 [==============================] - 5s 6ms/step - loss: 2865.3359 - mae: 9.6838 - val_loss: 3578.6282 - val_mae: 11.4627\n",
      "Epoch 124/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 2911.6428 - mae: 9.6478 - val_loss: 2415.4199 - val_mae: 9.1144\n",
      "Epoch 125/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 2949.9939 - mae: 9.4760 - val_loss: 4634.0864 - val_mae: 12.7001\n",
      "Epoch 126/500\n",
      "722/722 [==============================] - 8s 11ms/step - loss: 3810.6782 - mae: 11.0699 - val_loss: 3100.0276 - val_mae: 11.7500\n",
      "Epoch 127/500\n",
      "722/722 [==============================] - 7s 10ms/step - loss: 2909.3145 - mae: 9.9543 - val_loss: 1735.1851 - val_mae: 7.1343\n",
      "Epoch 128/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 3709.7363 - mae: 11.3229 - val_loss: 2676.5974 - val_mae: 8.4384\n",
      "Epoch 129/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 3216.7114 - mae: 9.9669 - val_loss: 12722.7852 - val_mae: 24.1892\n",
      "Epoch 130/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 3774.8757 - mae: 10.5518 - val_loss: 2123.3833 - val_mae: 9.2284\n",
      "Epoch 131/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 3472.1716 - mae: 10.3825 - val_loss: 2702.2202 - val_mae: 8.9919\n",
      "Epoch 132/500\n",
      "722/722 [==============================] - 9s 12ms/step - loss: 2624.1077 - mae: 8.9384 - val_loss: 2335.1631 - val_mae: 7.4429\n",
      "Epoch 133/500\n",
      "722/722 [==============================] - 7s 9ms/step - loss: 2207.0137 - mae: 8.8317 - val_loss: 4915.0981 - val_mae: 12.2842\n",
      "Epoch 134/500\n",
      "722/722 [==============================] - 5s 6ms/step - loss: 2934.4265 - mae: 9.5830 - val_loss: 2056.1250 - val_mae: 7.4163\n",
      "Epoch 135/500\n",
      "722/722 [==============================] - 7s 9ms/step - loss: 2964.9663 - mae: 9.0595 - val_loss: 1586.7924 - val_mae: 7.4315\n",
      "Epoch 136/500\n",
      "722/722 [==============================] - 10s 14ms/step - loss: 2806.7246 - mae: 9.4448 - val_loss: 1790.9077 - val_mae: 10.7753\n",
      "Epoch 137/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 4119.4419 - mae: 11.0872 - val_loss: 4128.5771 - val_mae: 17.6054\n",
      "Epoch 138/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 3461.1550 - mae: 10.6034 - val_loss: 3048.8792 - val_mae: 9.8686\n",
      "Epoch 139/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 2986.1260 - mae: 9.8535 - val_loss: 3739.7419 - val_mae: 10.3580\n",
      "Epoch 140/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 3993.3757 - mae: 11.2331 - val_loss: 3203.9292 - val_mae: 8.7784\n",
      "Epoch 141/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 2704.9285 - mae: 8.9772 - val_loss: 1819.5964 - val_mae: 8.2503\n",
      "Epoch 142/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 2973.1848 - mae: 9.4675 - val_loss: 3264.0388 - val_mae: 8.7142\n",
      "Epoch 143/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 3305.8147 - mae: 9.8577 - val_loss: 7606.1279 - val_mae: 14.3384\n",
      "Epoch 144/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 4333.1646 - mae: 11.2177 - val_loss: 10641.4014 - val_mae: 17.7781\n",
      "Epoch 145/500\n",
      "722/722 [==============================] - 7s 9ms/step - loss: 3034.7593 - mae: 9.8237 - val_loss: 2298.0642 - val_mae: 8.2345\n",
      "Epoch 146/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 3667.5491 - mae: 10.3947 - val_loss: 2978.3701 - val_mae: 12.0318\n",
      "Epoch 147/500\n",
      "722/722 [==============================] - 7s 9ms/step - loss: 3556.2080 - mae: 11.2590 - val_loss: 2218.1926 - val_mae: 8.5848\n",
      "Epoch 148/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 2401.2505 - mae: 9.1882 - val_loss: 5270.7993 - val_mae: 13.9353\n",
      "Epoch 149/500\n",
      "722/722 [==============================] - 6s 9ms/step - loss: 3672.1318 - mae: 10.8252 - val_loss: 6559.1494 - val_mae: 14.6924\n",
      "Epoch 150/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 3348.2275 - mae: 10.8919 - val_loss: 3261.8022 - val_mae: 10.8770\n",
      "Epoch 151/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 3297.3584 - mae: 9.9718 - val_loss: 5432.7896 - val_mae: 12.7596\n",
      "Epoch 152/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 3099.4456 - mae: 9.6844 - val_loss: 3875.7280 - val_mae: 13.6682\n",
      "Epoch 153/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 3043.9482 - mae: 9.8303 - val_loss: 1594.7200 - val_mae: 7.1525\n",
      "Epoch 154/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 3558.7295 - mae: 10.4513 - val_loss: 2482.7852 - val_mae: 9.7345\n",
      "Epoch 155/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 2376.3066 - mae: 8.5307 - val_loss: 1862.9150 - val_mae: 8.8050\n",
      "Epoch 156/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 2696.9414 - mae: 8.8568 - val_loss: 1646.2173 - val_mae: 7.3121\n",
      "Epoch 157/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 2660.8162 - mae: 9.3675 - val_loss: 1246.9221 - val_mae: 6.7164\n",
      "Epoch 158/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 4055.1277 - mae: 11.1997 - val_loss: 5781.1611 - val_mae: 22.2678\n",
      "Epoch 159/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 3031.1797 - mae: 10.0630 - val_loss: 3734.9829 - val_mae: 10.9405\n",
      "Epoch 160/500\n",
      "722/722 [==============================] - 8s 11ms/step - loss: 2926.3362 - mae: 9.7840 - val_loss: 15076.8389 - val_mae: 22.6861\n",
      "Epoch 161/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 3800.1421 - mae: 10.3658 - val_loss: 1639.0361 - val_mae: 6.9917\n",
      "Epoch 162/500\n",
      "722/722 [==============================] - 6s 9ms/step - loss: 2719.9133 - mae: 9.1015 - val_loss: 2100.1497 - val_mae: 6.9119\n",
      "Epoch 163/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 2892.6023 - mae: 9.8996 - val_loss: 1964.5408 - val_mae: 8.5324\n",
      "Epoch 164/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 2411.6501 - mae: 8.3652 - val_loss: 2030.6941 - val_mae: 7.4555\n",
      "Epoch 165/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 1979.1348 - mae: 8.4694 - val_loss: 2858.5232 - val_mae: 11.4807\n",
      "Epoch 166/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 3310.2444 - mae: 10.5356 - val_loss: 2250.2205 - val_mae: 7.9373\n",
      "Epoch 167/500\n",
      "722/722 [==============================] - 7s 10ms/step - loss: 2864.9226 - mae: 9.1677 - val_loss: 2331.5068 - val_mae: 10.4556\n",
      "Epoch 168/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 3539.1033 - mae: 11.1384 - val_loss: 1446.7811 - val_mae: 6.7677\n",
      "Epoch 169/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 2370.2070 - mae: 8.4225 - val_loss: 2664.4917 - val_mae: 13.6061\n",
      "Epoch 170/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 3115.6775 - mae: 9.8458 - val_loss: 9241.3184 - val_mae: 18.2007\n",
      "Epoch 171/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 1871.2048 - mae: 7.9668 - val_loss: 5186.6685 - val_mae: 13.9228\n",
      "Epoch 172/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 3943.0596 - mae: 11.4812 - val_loss: 9628.0625 - val_mae: 23.7870\n",
      "Epoch 173/500\n",
      "722/722 [==============================] - 9s 13ms/step - loss: 3187.2188 - mae: 10.1763 - val_loss: 1387.3027 - val_mae: 7.8130\n",
      "Epoch 174/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 2327.4756 - mae: 8.6480 - val_loss: 1828.9523 - val_mae: 8.5135\n",
      "Epoch 175/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 2868.4143 - mae: 9.6929 - val_loss: 2001.2502 - val_mae: 9.5172\n",
      "Epoch 176/500\n",
      "722/722 [==============================] - 7s 9ms/step - loss: 3405.7200 - mae: 10.1693 - val_loss: 2860.0776 - val_mae: 10.4515\n",
      "Epoch 177/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 3572.1584 - mae: 10.6730 - val_loss: 4680.5776 - val_mae: 11.0974\n",
      "Epoch 178/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 2798.3142 - mae: 9.4817 - val_loss: 2022.5264 - val_mae: 8.9675\n",
      "Epoch 179/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 3418.0972 - mae: 10.0417 - val_loss: 1190.3995 - val_mae: 6.4186\n",
      "Epoch 180/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 2404.7791 - mae: 9.1393 - val_loss: 1677.6501 - val_mae: 9.3463\n",
      "Epoch 181/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 2248.7661 - mae: 8.9169 - val_loss: 1098.0989 - val_mae: 6.6740\n",
      "Epoch 182/500\n",
      "722/722 [==============================] - 5s 6ms/step - loss: 3279.7070 - mae: 9.8688 - val_loss: 2149.8091 - val_mae: 9.3007\n",
      "Epoch 183/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 2455.8115 - mae: 8.2471 - val_loss: 3593.0408 - val_mae: 11.9748\n",
      "Epoch 184/500\n",
      "722/722 [==============================] - 7s 9ms/step - loss: 2676.9636 - mae: 8.8698 - val_loss: 6545.1885 - val_mae: 15.4256\n",
      "Epoch 185/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 5390.6650 - mae: 13.0814 - val_loss: 3380.5098 - val_mae: 13.0963\n",
      "Epoch 186/500\n",
      "722/722 [==============================] - 6s 9ms/step - loss: 3068.9597 - mae: 10.5755 - val_loss: 2087.0217 - val_mae: 8.4364\n",
      "Epoch 187/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 3160.5208 - mae: 10.7211 - val_loss: 4722.9644 - val_mae: 15.7547\n",
      "Epoch 188/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 2796.5955 - mae: 9.2774 - val_loss: 6327.3008 - val_mae: 17.5524\n",
      "Epoch 189/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 3516.0071 - mae: 10.3387 - val_loss: 1881.8120 - val_mae: 9.3751\n",
      "Epoch 190/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 2055.9839 - mae: 8.7328 - val_loss: 9757.9082 - val_mae: 17.5932\n",
      "Epoch 191/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 3533.6577 - mae: 10.7339 - val_loss: 5320.0405 - val_mae: 10.5896\n",
      "Epoch 192/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 2982.2080 - mae: 9.7307 - val_loss: 6116.9917 - val_mae: 12.5690\n",
      "Epoch 193/500\n",
      "722/722 [==============================] - 5s 6ms/step - loss: 2060.8716 - mae: 8.2560 - val_loss: 2160.1982 - val_mae: 9.5826\n",
      "Epoch 194/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 2005.2159 - mae: 7.8260 - val_loss: 2753.8855 - val_mae: 11.6646\n",
      "Epoch 195/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 2097.2092 - mae: 8.0488 - val_loss: 1321.7388 - val_mae: 6.0077\n",
      "Epoch 196/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 5603.8833 - mae: 12.4787 - val_loss: 5557.8237 - val_mae: 17.2097\n",
      "Epoch 197/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 3279.0413 - mae: 11.8868 - val_loss: 3571.1191 - val_mae: 13.4895\n",
      "Epoch 198/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 2958.6450 - mae: 10.5231 - val_loss: 1123.8009 - val_mae: 7.3281\n",
      "Epoch 199/500\n",
      "722/722 [==============================] - 7s 9ms/step - loss: 2413.1997 - mae: 9.6412 - val_loss: 1014.3923 - val_mae: 7.7962\n",
      "Epoch 200/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 2592.8655 - mae: 9.2935 - val_loss: 1116.8137 - val_mae: 6.8115\n",
      "Epoch 201/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 2418.2087 - mae: 9.5269 - val_loss: 890.8658 - val_mae: 6.5942\n",
      "Epoch 202/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 2942.7083 - mae: 10.1829 - val_loss: 1825.4358 - val_mae: 9.2168\n",
      "Epoch 203/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 2350.7949 - mae: 8.8054 - val_loss: 8136.3706 - val_mae: 18.2429\n",
      "Epoch 204/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 3187.9753 - mae: 10.2961 - val_loss: 1286.1804 - val_mae: 6.7708\n",
      "Epoch 205/500\n",
      "722/722 [==============================] - 5s 6ms/step - loss: 1790.8822 - mae: 7.8173 - val_loss: 970.1600 - val_mae: 5.7601\n",
      "Epoch 206/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 2962.2947 - mae: 9.6335 - val_loss: 2564.4924 - val_mae: 11.4021\n",
      "Epoch 207/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 2834.4895 - mae: 8.8179 - val_loss: 13415.0605 - val_mae: 23.7347\n",
      "Epoch 208/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 2250.4463 - mae: 9.1621 - val_loss: 1193.7561 - val_mae: 7.8222\n",
      "Epoch 209/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 2165.3889 - mae: 8.4981 - val_loss: 991.3231 - val_mae: 6.7977\n",
      "Epoch 210/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 3252.1650 - mae: 9.9881 - val_loss: 2065.6958 - val_mae: 8.1531\n",
      "Epoch 211/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 3217.4888 - mae: 9.8475 - val_loss: 4873.1470 - val_mae: 18.3897\n",
      "Epoch 212/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 2097.5242 - mae: 9.2454 - val_loss: 1922.1337 - val_mae: 10.3607\n",
      "Epoch 213/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 2129.9856 - mae: 8.9401 - val_loss: 742.5141 - val_mae: 5.9285\n",
      "Epoch 214/500\n",
      "722/722 [==============================] - 7s 10ms/step - loss: 3023.3726 - mae: 9.7402 - val_loss: 4433.1201 - val_mae: 11.6665\n",
      "Epoch 215/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 3209.1384 - mae: 10.3465 - val_loss: 2379.2161 - val_mae: 9.6908\n",
      "Epoch 216/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 2352.8630 - mae: 8.8422 - val_loss: 4275.4966 - val_mae: 11.6565\n",
      "Epoch 217/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 1476.8573 - mae: 7.2539 - val_loss: 732.7495 - val_mae: 5.2756\n",
      "Epoch 218/500\n",
      "722/722 [==============================] - 5s 6ms/step - loss: 3304.6135 - mae: 10.6038 - val_loss: 1024.7240 - val_mae: 6.4939\n",
      "Epoch 219/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 2038.5089 - mae: 7.8654 - val_loss: 994.8510 - val_mae: 5.9046\n",
      "Epoch 220/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 2759.7019 - mae: 9.5494 - val_loss: 3199.6243 - val_mae: 9.5248\n",
      "Epoch 221/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 3141.5613 - mae: 9.9259 - val_loss: 2137.4766 - val_mae: 9.5813\n",
      "Epoch 222/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 2103.4563 - mae: 8.8382 - val_loss: 4468.9028 - val_mae: 15.0240\n",
      "Epoch 223/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 2925.1724 - mae: 9.7882 - val_loss: 1559.2791 - val_mae: 7.4217\n",
      "Epoch 224/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 1901.4696 - mae: 7.9605 - val_loss: 677.7565 - val_mae: 5.1790\n",
      "Epoch 225/500\n",
      "722/722 [==============================] - 5s 6ms/step - loss: 2330.6946 - mae: 8.9945 - val_loss: 12649.3975 - val_mae: 17.6670\n",
      "Epoch 226/500\n",
      "722/722 [==============================] - 5s 8ms/step - loss: 3268.3826 - mae: 10.0288 - val_loss: 2832.2754 - val_mae: 8.7592\n",
      "Epoch 227/500\n",
      "722/722 [==============================] - 5s 6ms/step - loss: 2183.3103 - mae: 8.8409 - val_loss: 725.1535 - val_mae: 5.2625\n",
      "Epoch 228/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 1860.6125 - mae: 7.5826 - val_loss: 3283.0398 - val_mae: 10.0448\n",
      "Epoch 229/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 2246.5630 - mae: 8.0351 - val_loss: 654.1973 - val_mae: 5.3453\n",
      "Epoch 230/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 2450.2095 - mae: 9.0817 - val_loss: 2292.4302 - val_mae: 8.9748\n",
      "Epoch 231/500\n",
      "722/722 [==============================] - 7s 9ms/step - loss: 1865.3979 - mae: 7.9248 - val_loss: 947.3273 - val_mae: 6.9173\n",
      "Epoch 232/500\n",
      "722/722 [==============================] - 6s 9ms/step - loss: 3785.3257 - mae: 10.8601 - val_loss: 2265.9180 - val_mae: 11.3778\n",
      "Epoch 233/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 2212.0854 - mae: 8.8783 - val_loss: 777.4983 - val_mae: 5.6373\n",
      "Epoch 234/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 1773.5433 - mae: 7.7059 - val_loss: 2052.7595 - val_mae: 10.0006\n",
      "Epoch 235/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 2120.1697 - mae: 8.3110 - val_loss: 3240.0588 - val_mae: 15.9864\n",
      "Epoch 236/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 3360.5322 - mae: 10.6237 - val_loss: 871.7177 - val_mae: 5.6006\n",
      "Epoch 237/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 2941.2134 - mae: 9.9870 - val_loss: 11204.8877 - val_mae: 16.4139\n",
      "Epoch 238/500\n",
      "722/722 [==============================] - 7s 9ms/step - loss: 2005.3713 - mae: 8.3685 - val_loss: 1866.9735 - val_mae: 9.4775\n",
      "Epoch 239/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 2943.4629 - mae: 9.4265 - val_loss: 1260.7616 - val_mae: 7.4987\n",
      "Epoch 240/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 1565.7379 - mae: 7.3287 - val_loss: 11456.2568 - val_mae: 17.6590\n",
      "Epoch 241/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 3222.6345 - mae: 10.3632 - val_loss: 1425.4443 - val_mae: 7.3680\n",
      "Epoch 242/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 1725.5712 - mae: 7.6724 - val_loss: 968.1606 - val_mae: 6.2899\n",
      "Epoch 243/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 3208.8064 - mae: 9.7342 - val_loss: 1568.5675 - val_mae: 6.9943\n",
      "Epoch 244/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 2469.4563 - mae: 8.8265 - val_loss: 984.3069 - val_mae: 6.9529\n",
      "Epoch 245/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 2863.8345 - mae: 9.8017 - val_loss: 1515.0217 - val_mae: 7.9775\n",
      "Epoch 246/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 3162.2688 - mae: 10.0020 - val_loss: 1847.0247 - val_mae: 7.8169\n",
      "Epoch 247/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 2657.8962 - mae: 8.4775 - val_loss: 1305.6313 - val_mae: 6.6074\n",
      "Epoch 248/500\n",
      "722/722 [==============================] - 7s 10ms/step - loss: 1374.0038 - mae: 6.9653 - val_loss: 792.7230 - val_mae: 6.1697\n",
      "Epoch 249/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 2927.3376 - mae: 10.0068 - val_loss: 842.7095 - val_mae: 5.7521\n",
      "Epoch 250/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 2553.6799 - mae: 9.4655 - val_loss: 755.1653 - val_mae: 5.9447\n",
      "Epoch 251/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 3190.3657 - mae: 9.9517 - val_loss: 1479.9036 - val_mae: 8.5864\n",
      "Epoch 252/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 3374.4033 - mae: 10.3974 - val_loss: 1726.3486 - val_mae: 8.9701\n",
      "Epoch 253/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 2127.6484 - mae: 8.1247 - val_loss: 1644.3274 - val_mae: 6.7007\n",
      "Epoch 254/500\n",
      "722/722 [==============================] - 9s 12ms/step - loss: 1987.1011 - mae: 7.9051 - val_loss: 581.7175 - val_mae: 5.2631\n",
      "Epoch 255/500\n",
      "722/722 [==============================] - 5s 6ms/step - loss: 2005.7620 - mae: 7.9349 - val_loss: 2980.5513 - val_mae: 11.8404\n",
      "Epoch 256/500\n",
      "722/722 [==============================] - 8s 11ms/step - loss: 2210.9226 - mae: 8.6758 - val_loss: 1856.2490 - val_mae: 7.3726\n",
      "Epoch 257/500\n",
      "722/722 [==============================] - 7s 9ms/step - loss: 3123.5469 - mae: 10.0825 - val_loss: 1094.0712 - val_mae: 7.1159\n",
      "Epoch 258/500\n",
      "722/722 [==============================] - 11s 15ms/step - loss: 1596.9153 - mae: 7.1506 - val_loss: 6040.2690 - val_mae: 19.5502\n",
      "Epoch 259/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 2798.3782 - mae: 9.0574 - val_loss: 1391.9247 - val_mae: 7.1696\n",
      "Epoch 260/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 2819.5864 - mae: 8.9607 - val_loss: 1398.3682 - val_mae: 7.1588\n",
      "Epoch 261/500\n",
      "722/722 [==============================] - 5s 6ms/step - loss: 2292.6794 - mae: 8.4500 - val_loss: 1073.8407 - val_mae: 6.9675\n",
      "Epoch 262/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 2197.7866 - mae: 8.7692 - val_loss: 1053.8038 - val_mae: 8.3422\n",
      "Epoch 263/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 2604.5090 - mae: 9.6185 - val_loss: 4211.2480 - val_mae: 15.0610\n",
      "Epoch 264/500\n",
      "722/722 [==============================] - 6s 9ms/step - loss: 2534.6025 - mae: 9.0286 - val_loss: 2839.9158 - val_mae: 9.3689\n",
      "Epoch 265/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 1586.5920 - mae: 7.2433 - val_loss: 564.5843 - val_mae: 5.2906\n",
      "Epoch 266/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 2411.8086 - mae: 8.2427 - val_loss: 1630.4583 - val_mae: 9.2825\n",
      "Epoch 267/500\n",
      "722/722 [==============================] - 5s 6ms/step - loss: 1842.4032 - mae: 7.4040 - val_loss: 538.9852 - val_mae: 5.1219\n",
      "Epoch 268/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 2913.8096 - mae: 10.0850 - val_loss: 672.4305 - val_mae: 5.9198\n",
      "Epoch 269/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 1589.6774 - mae: 7.4635 - val_loss: 1929.2656 - val_mae: 8.0332\n",
      "Epoch 270/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 1686.1621 - mae: 7.3491 - val_loss: 2644.0969 - val_mae: 8.7567\n",
      "Epoch 271/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 2047.6484 - mae: 7.8614 - val_loss: 1317.8553 - val_mae: 6.7251\n",
      "Epoch 272/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 2218.3406 - mae: 8.7574 - val_loss: 2643.9836 - val_mae: 9.5738\n",
      "Epoch 273/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 1505.9269 - mae: 6.9818 - val_loss: 1847.8401 - val_mae: 9.5333\n",
      "Epoch 274/500\n",
      "722/722 [==============================] - 5s 6ms/step - loss: 1496.8224 - mae: 7.4618 - val_loss: 1129.9993 - val_mae: 5.7348\n",
      "Epoch 275/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 1680.0631 - mae: 7.5637 - val_loss: 2099.4636 - val_mae: 8.5812\n",
      "Epoch 276/500\n",
      "722/722 [==============================] - 7s 10ms/step - loss: 2168.3582 - mae: 8.4934 - val_loss: 796.9678 - val_mae: 5.3317\n",
      "Epoch 277/500\n",
      "722/722 [==============================] - 8s 12ms/step - loss: 4694.5513 - mae: 12.4105 - val_loss: 1251.8197 - val_mae: 8.4340\n",
      "Epoch 278/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 1736.4080 - mae: 8.1943 - val_loss: 694.7584 - val_mae: 5.7312\n",
      "Epoch 279/500\n",
      "722/722 [==============================] - 6s 9ms/step - loss: 2228.1504 - mae: 8.1605 - val_loss: 1479.7572 - val_mae: 6.9797\n",
      "Epoch 280/500\n",
      "722/722 [==============================] - 8s 11ms/step - loss: 2723.6169 - mae: 8.7278 - val_loss: 6174.7231 - val_mae: 13.8698\n",
      "Epoch 281/500\n",
      "722/722 [==============================] - 7s 10ms/step - loss: 2024.4667 - mae: 7.8065 - val_loss: 757.7828 - val_mae: 6.3481\n",
      "Epoch 282/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 1569.1278 - mae: 7.4266 - val_loss: 676.0140 - val_mae: 6.2339\n",
      "Epoch 283/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 2957.3098 - mae: 9.5241 - val_loss: 2658.5105 - val_mae: 7.5800\n",
      "Epoch 284/500\n",
      "722/722 [==============================] - 8s 11ms/step - loss: 1598.3806 - mae: 7.5632 - val_loss: 1656.4473 - val_mae: 7.1015\n",
      "Epoch 285/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 3106.8359 - mae: 10.1850 - val_loss: 2936.0156 - val_mae: 11.1861\n",
      "Epoch 286/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 1335.8444 - mae: 6.8539 - val_loss: 2454.0056 - val_mae: 8.8258\n",
      "Epoch 287/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 2135.5344 - mae: 7.9430 - val_loss: 6248.1812 - val_mae: 12.2469\n",
      "Epoch 288/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 2501.9541 - mae: 8.4364 - val_loss: 976.7061 - val_mae: 5.9969\n",
      "Epoch 289/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 2083.7402 - mae: 8.5085 - val_loss: 2250.8884 - val_mae: 10.2661\n",
      "Epoch 290/500\n",
      "722/722 [==============================] - 7s 10ms/step - loss: 1568.5444 - mae: 7.8326 - val_loss: 1269.5635 - val_mae: 6.9948\n",
      "Epoch 291/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 1653.9514 - mae: 7.1830 - val_loss: 907.1533 - val_mae: 6.7142\n",
      "Epoch 292/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 1753.1254 - mae: 7.5822 - val_loss: 2724.9988 - val_mae: 10.4630\n",
      "Epoch 293/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 1740.2932 - mae: 7.3945 - val_loss: 3067.8794 - val_mae: 12.7257\n",
      "Epoch 294/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 1920.8888 - mae: 7.4002 - val_loss: 828.2582 - val_mae: 5.8564\n",
      "Epoch 295/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 1842.3075 - mae: 7.6046 - val_loss: 556.8899 - val_mae: 4.9755\n",
      "Epoch 296/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 1094.8755 - mae: 6.1569 - val_loss: 1962.7170 - val_mae: 8.8404\n",
      "Epoch 297/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 3359.7205 - mae: 10.1195 - val_loss: 984.7462 - val_mae: 7.1332\n",
      "Epoch 298/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 3266.9370 - mae: 9.8147 - val_loss: 1527.0868 - val_mae: 9.1514\n",
      "Epoch 299/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 1561.8962 - mae: 7.6037 - val_loss: 1618.1547 - val_mae: 8.8819\n",
      "Epoch 300/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 1651.4596 - mae: 7.3272 - val_loss: 1107.9479 - val_mae: 6.5349\n",
      "Epoch 301/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 1970.2887 - mae: 7.5127 - val_loss: 4499.8169 - val_mae: 12.8542\n",
      "Epoch 302/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 1959.5037 - mae: 7.7835 - val_loss: 811.9352 - val_mae: 5.6323\n",
      "Epoch 303/500\n",
      "722/722 [==============================] - 6s 9ms/step - loss: 2233.0522 - mae: 8.3230 - val_loss: 1982.4379 - val_mae: 8.5109\n",
      "Epoch 304/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 2597.6768 - mae: 8.9805 - val_loss: 5789.0781 - val_mae: 16.9680\n",
      "Epoch 305/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 2070.5454 - mae: 8.1929 - val_loss: 2184.2734 - val_mae: 9.7399\n",
      "Epoch 306/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 2422.5706 - mae: 8.8125 - val_loss: 2321.6682 - val_mae: 9.6158\n",
      "Epoch 307/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 1957.9061 - mae: 7.4077 - val_loss: 3463.1753 - val_mae: 11.6515\n",
      "Epoch 308/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 1992.8203 - mae: 8.4948 - val_loss: 4373.1328 - val_mae: 13.4778\n",
      "Epoch 309/500\n",
      "722/722 [==============================] - 6s 9ms/step - loss: 2243.3098 - mae: 7.8181 - val_loss: 453.6602 - val_mae: 4.9076\n",
      "Epoch 310/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 1841.7501 - mae: 7.3491 - val_loss: 849.8729 - val_mae: 5.1530\n",
      "Epoch 311/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 2858.5437 - mae: 9.1042 - val_loss: 1159.5284 - val_mae: 6.4417\n",
      "Epoch 312/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 1008.6736 - mae: 6.0545 - val_loss: 318.4579 - val_mae: 3.7105\n",
      "Epoch 313/500\n",
      "722/722 [==============================] - 7s 9ms/step - loss: 1503.3186 - mae: 6.8129 - val_loss: 1023.7793 - val_mae: 7.9105\n",
      "Epoch 314/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 1314.0049 - mae: 6.6360 - val_loss: 1656.7910 - val_mae: 5.7470\n",
      "Epoch 315/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 2026.8883 - mae: 8.1115 - val_loss: 4063.8955 - val_mae: 14.4326\n",
      "Epoch 316/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 2163.8464 - mae: 7.9693 - val_loss: 5754.5894 - val_mae: 18.6502\n",
      "Epoch 317/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 1596.7062 - mae: 7.0785 - val_loss: 760.0746 - val_mae: 5.5774\n",
      "Epoch 318/500\n",
      "722/722 [==============================] - 8s 12ms/step - loss: 1187.8037 - mae: 6.2819 - val_loss: 2277.9419 - val_mae: 7.5488\n",
      "Epoch 319/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 2024.2657 - mae: 7.7544 - val_loss: 571.1462 - val_mae: 5.9279\n",
      "Epoch 320/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 2150.3840 - mae: 7.7544 - val_loss: 1027.6952 - val_mae: 6.6117\n",
      "Epoch 321/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 1723.1023 - mae: 7.5461 - val_loss: 2481.9243 - val_mae: 9.2685\n",
      "Epoch 322/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 1867.6460 - mae: 7.7188 - val_loss: 572.3289 - val_mae: 5.9022\n",
      "Epoch 323/500\n",
      "722/722 [==============================] - 6s 8ms/step - loss: 1620.4445 - mae: 6.6903 - val_loss: 2277.6343 - val_mae: 8.7541\n",
      "Epoch 324/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 3241.4978 - mae: 9.7280 - val_loss: 2312.4041 - val_mae: 7.8082\n",
      "Epoch 325/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 2037.0936 - mae: 7.6955 - val_loss: 6825.8398 - val_mae: 15.3802\n",
      "Epoch 326/500\n",
      "722/722 [==============================] - 7s 9ms/step - loss: 3315.9797 - mae: 9.5564 - val_loss: 2563.3130 - val_mae: 11.0835\n",
      "Epoch 327/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 1523.2996 - mae: 7.1482 - val_loss: 927.4470 - val_mae: 6.4111\n",
      "Epoch 328/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 1716.7893 - mae: 7.2889 - val_loss: 771.0673 - val_mae: 5.6846\n",
      "Epoch 329/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 2761.6064 - mae: 8.6419 - val_loss: 1420.8536 - val_mae: 7.2559\n",
      "Epoch 330/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 1352.7544 - mae: 6.6333 - val_loss: 939.5159 - val_mae: 6.8569\n",
      "Epoch 331/500\n",
      "722/722 [==============================] - 11s 15ms/step - loss: 1402.8025 - mae: 6.9184 - val_loss: 1614.9053 - val_mae: 9.0255\n",
      "Epoch 332/500\n",
      "722/722 [==============================] - 7s 9ms/step - loss: 2154.8892 - mae: 7.8031 - val_loss: 2371.3628 - val_mae: 8.4537\n",
      "Epoch 333/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 1899.8458 - mae: 7.9228 - val_loss: 1021.4382 - val_mae: 5.7884\n",
      "Epoch 334/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 1309.1722 - mae: 6.4457 - val_loss: 935.2759 - val_mae: 6.5977\n",
      "Epoch 335/500\n",
      "722/722 [==============================] - 7s 10ms/step - loss: 2069.7744 - mae: 7.1920 - val_loss: 466.7933 - val_mae: 5.2475\n",
      "Epoch 336/500\n",
      "722/722 [==============================] - 6s 9ms/step - loss: 1532.7666 - mae: 6.8048 - val_loss: 4914.8604 - val_mae: 14.2956\n",
      "Epoch 337/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 1755.7339 - mae: 7.2367 - val_loss: 786.2184 - val_mae: 5.7935\n",
      "Epoch 338/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 1831.9058 - mae: 7.5528 - val_loss: 567.6312 - val_mae: 4.2592\n",
      "Epoch 339/500\n",
      "722/722 [==============================] - 5s 8ms/step - loss: 1199.5297 - mae: 6.0983 - val_loss: 363.2526 - val_mae: 4.1276\n",
      "Epoch 340/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 1157.2197 - mae: 5.7824 - val_loss: 713.5084 - val_mae: 6.0332\n",
      "Epoch 341/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 2624.0786 - mae: 8.3781 - val_loss: 2813.6538 - val_mae: 12.3325\n",
      "Epoch 342/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 1867.1895 - mae: 7.7462 - val_loss: 4199.4141 - val_mae: 12.4185\n",
      "Epoch 343/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 1088.8127 - mae: 6.1889 - val_loss: 780.9511 - val_mae: 5.2999\n",
      "Epoch 344/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 2588.6938 - mae: 8.6164 - val_loss: 473.4211 - val_mae: 4.3545\n",
      "Epoch 345/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 2760.4453 - mae: 8.6622 - val_loss: 1305.3413 - val_mae: 8.5044\n",
      "Epoch 346/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 1610.6807 - mae: 7.1306 - val_loss: 3062.5635 - val_mae: 10.6972\n",
      "Epoch 347/500\n",
      "722/722 [==============================] - 7s 10ms/step - loss: 1369.4270 - mae: 6.9974 - val_loss: 677.3015 - val_mae: 5.9408\n",
      "Epoch 348/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 1437.6350 - mae: 6.4802 - val_loss: 3337.0779 - val_mae: 10.8123\n",
      "Epoch 349/500\n",
      "722/722 [==============================] - 5s 7ms/step - loss: 1988.8406 - mae: 7.7623 - val_loss: 854.1205 - val_mae: 7.0188\n",
      "Epoch 350/500\n",
      "722/722 [==============================] - 3s 4ms/step - loss: 1695.0972 - mae: 7.2798 - val_loss: 939.9256 - val_mae: 5.9722\n",
      "Epoch 351/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 1872.1232 - mae: 7.7196 - val_loss: 1078.6537 - val_mae: 6.9535\n",
      "Epoch 352/500\n",
      "722/722 [==============================] - 4s 5ms/step - loss: 1646.3618 - mae: 7.0149 - val_loss: 812.5914 - val_mae: 7.0671\n",
      "Epoch 353/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 2272.6399 - mae: 8.7050 - val_loss: 1325.1895 - val_mae: 8.5415\n",
      "Epoch 354/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 1867.7234 - mae: 7.5149 - val_loss: 507.5014 - val_mae: 4.8956\n",
      "Epoch 355/500\n",
      "722/722 [==============================] - 3s 5ms/step - loss: 1840.2871 - mae: 7.5159 - val_loss: 1653.5332 - val_mae: 7.7100\n",
      "Epoch 356/500\n",
      "722/722 [==============================] - 4s 6ms/step - loss: 1360.8944 - mae: 6.5592 - val_loss: 492.6856 - val_mae: 4.2611\n",
      "Epoch 357/500\n",
      "359/722 [=============>................] - ETA: 5s - loss: 1106.6177 - mae: 6.3940"
     ]
    }
   ],
   "source": [
    "history = ann.fit(X_train, y_train,\n",
    "                    validation_data = (X_val, y_val),\n",
    "                    callbacks=[es],\n",
    "                    epochs=500,\n",
    "                    batch_size=30,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# let's see the training and validation accuracy by epoch\n",
    "history_dict = history.history\n",
    "loss_values = history_dict['loss'] # you can change this\n",
    "val_loss_values = history_dict['val_loss'] # you can also change this\n",
    "epochs = range(1, len(loss_values) + 1) # range of X (no. of epochs)\n",
    "\n",
    "# Set global font to Times New Roman and font size\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "# Create a plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(epochs, loss_values, 'blue', label='Train set')\n",
    "plt.plot(epochs, val_loss_values, 'orange', label='Validation set')\n",
    "#plt.title('Training and testing loss')\n",
    "\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.savefig('Section1_ANN3_2.png', dpi=200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "loss_values1 = history_dict['mae'] # you can change this\n",
    "val_loss_values1 = history_dict['val_mae'] # you can also change this\n",
    "epochs = range(1, len(loss_values1) + 1) # range of X (no. of epochs)\n",
    "# Create a plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(epochs, loss_values1, 'blue', label='Train set')\n",
    "plt.plot(epochs, val_loss_values1, 'orange', label='Validation set')\n",
    "#plt.title('Training and testing MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE (kPa)')\n",
    "plt.legend()\n",
    "plt.savefig('Section2_ANN3_3.png', dpi=200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "ann.save('Section1_ANN_3')\n",
    "import pickle\n",
    "pickle.dump(sc, open('Section1_scaler_ANN_3.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 - Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# scatterplot of actual vs. pred\n",
    "# specify the dimensions \n",
    "fig, axes = plt.subplots(1,2) # 1 row, 2 columns\n",
    "\n",
    "# this makes the individual subplots\n",
    "# Training Results\n",
    "axes[0].scatter(x=y_train, y=ann.predict(X_train)) #first row, first entry (left top)\n",
    "axes[0].set_xlabel(\"Actual\", fontsize=10)\n",
    "axes[0].set_ylabel(\"Predicted\",  fontsize=10)\n",
    "axes[0].set_title(\"Training\")\n",
    "# add 45 deg line\n",
    "x = np.linspace(*axes[0].get_xlim())\n",
    "axes[0].plot(x, x, color='red')\n",
    "# Validation Results\n",
    "axes[1].scatter(x=y_val, y=ann.predict(X_val)) # first row, second entry (right top)\n",
    "axes[1].set_xlabel(\"Actual\", fontsize=10)\n",
    "axes[1].set_ylabel(\"Predicted\",  fontsize=10)\n",
    "axes[1].set_title(\"Validation\")\n",
    "# add 45 deg line\n",
    "x = np.linspace(*axes[1].get_xlim())\n",
    "axes[1].plot(x, x, color='red')\n",
    "\n",
    "# tight layout\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('Section1_ANN3_1.png', dpi=150, bbox_inches='tight')\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "IA0yApEmBG1X",
    "outputId": "cb981e1f-9204-4a2a-fece-9d66a6919189",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "y_pred = ann.predict(X_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(y_test, y_pred, color=\"blue\")\n",
    "plt.xlabel ('Actual data')\n",
    "plt.ylabel ('Predicted data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "# Initialize layout\n",
    "fig1, ax1 = plt.subplots(figsize = (6, 6))\n",
    "\n",
    "b_l = -200\n",
    "u_l = 4500\n",
    "\n",
    "# Add scatterplot\n",
    "ax1.scatter(y_test, y_pred, s=70, alpha=1, edgecolors=\"k\",c='mediumblue',zorder=5, label = 'Points')\n",
    "\n",
    "\n",
    "plt.ylabel('Predicted Incident Overpressure (kPa)')\n",
    "plt.xlabel('Target Incident Overpressure (kPa)')\n",
    "plt.xticks(fontsize = 19)\n",
    "plt.yticks(fontsize = 19)\n",
    "\n",
    "\n",
    "x1 = np.linspace(b_l, u_l)\n",
    "\n",
    "plt.plot(x1, x1, 'Red', label='45\\N{DEGREE SIGN} line',lw=2.5,alpha=1)\n",
    "\n",
    "plt.ylim(b_l, u_l)\n",
    "plt.xlim(b_l, u_l)\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=4, frameon = False, fontsize = 20)\n",
    "\n",
    "#plt.grid()\n",
    "\n",
    "ax1.spines['left'].set_color('black')        # setting up Y-axis tick color to red\n",
    "ax1.spines['bottom'].set_color('black')         #setting up above X-axis tick color to red\n",
    "\n",
    "plt.savefig('Section1_ANN3.png', dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# metrics\n",
    "trainpreds = ann.predict(X_train)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(mean_absolute_error(y_train, trainpreds)) # train\n",
    "print(mean_absolute_error(y_test, y_pred)) # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print (r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "r2_train = r2_score(y_train, trainpreds)\n",
    "print (r2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "count0 = 0\n",
    "for i in range(len(y_pred)):\n",
    "    line1 = y_pred[i] - 0.95*y_test[i]\n",
    "    line2 = y_pred[i] - 1.05*y_test[i]\n",
    "    mask0 = (line1 > 0) & (line2 < 0)\n",
    "    count0 = np.sum(mask0+count0)\n",
    "\n",
    "count1 = 0\n",
    "for i in range(len(y_pred)):\n",
    "    line1 = y_pred[i] - 0.9*y_test[i]\n",
    "    line2 = y_pred[i] - 1.1*y_test[i]\n",
    "    mask1 = (line1 > 0) & (line2 < 0)\n",
    "    count1 = np.sum(mask1+count1)\n",
    "\n",
    "count2 = 0\n",
    "for j in range(len(y_pred)):\n",
    "    line3 = y_pred[j] - 0.8*y_test[j]\n",
    "    line4 = y_pred[j] - 1.2*y_test[j]\n",
    "    mask2 = (line3 > 0) & (line4 < 0)\n",
    "    count2 = np.sum(mask2+count2)\n",
    "\n",
    "\n",
    "count3 = 0    \n",
    "for k in range(len(y_pred)):\n",
    "    line5 = y_pred[k] - 0.8*y_test[k]\n",
    "    line6 = y_pred[k] - 1.2*y_test[k]\n",
    "    mask3 = (line5 < 0) or (line6 > 0)\n",
    "    count3 = np.sum(mask3+count3)\n",
    "   \n",
    "\n",
    "print ('Within 5% margin', format((count0/len (y_pred)),'.2%'))\n",
    "print ('Within 10% margin', format((count1/len (y_pred)),'.2%'))\n",
    "print ('Within 20% margin', format((count2/len (y_pred)),'.2%'))\n",
    "print ('Out of 20% margin', format((count3/len (y_pred)),'.2%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "adj_r2 = 1 - ((1 - r2) * (len(X_test) - 1) / (len(X_test) - len (X_test[0]) - 1))\n",
    "print (adj_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "math.sqrt(mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mean_absolute_error(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Artificial Neural Network",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
